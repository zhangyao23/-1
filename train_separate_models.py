#!/usr/bin/env python3
"""
åˆ†åˆ«è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹å’Œå¼‚å¸¸åˆ†ç±»æ¨¡å‹
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import joblib

# å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆäºŒåˆ†ç±»ï¼‰
class AnomalyDetector(nn.Module):
    def __init__(self):
        super(AnomalyDetector, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(11, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 2)
        )
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x):
        return self.layers(x)

# å¼‚å¸¸åˆ†ç±»æ¨¡å‹ï¼ˆå…­åˆ†ç±»ï¼‰
class AnomalyClassifier(nn.Module):
    def __init__(self):
        super(AnomalyClassifier, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(11, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 6)
        )
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x):
        return self.layers(x)

def generate_data():
    print("ğŸ”„ Generating data...")
    from train_realistic_end_to_end_networks import generate_realistic_network_data
    
    X, y_binary, y_multiclass = generate_realistic_network_data(n_samples=30000)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, 'separate_models_scaler.pkl')
    print("âœ… Scaler saved to separate_models_scaler.pkl")
    
    return X_scaled, y_binary, y_multiclass

def train_detector(X, y_binary):
    print("\nğŸš€ Training Anomaly Detector...")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_binary, test_size=0.2, random_state=42, stratify=y_binary
    )
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    # è®­ç»ƒæ¨¡å‹
    model = AnomalyDetector()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    
    for epoch in range(100):
        model.train()
        total_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch + 1) % 20 == 0:
            model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for batch_x, batch_y in test_loader:
                    output = model(batch_x)
                    _, predicted = torch.max(output.data, 1)
                    total += batch_y.size(0)
                    correct += (predicted == batch_y).sum().item()
            accuracy = 100 * correct / total
            print(f"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Acc: {accuracy:.2f}%")
    
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'anomaly_detector.pth')
    print("âœ… Anomaly detector saved as anomaly_detector.pth")
    return model

def train_classifier(X, y_multiclass):
    print("\nğŸš€ Training Anomaly Classifier...")
    
    # åªä½¿ç”¨å¼‚å¸¸æ ·æœ¬
    anomaly_indices = np.where(y_multiclass > 0)[0]
    X_anomaly = X[anomaly_indices]
    y_anomaly = y_multiclass[anomaly_indices] - 1  # è½¬æ¢ä¸º0-5
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_anomaly, y_anomaly, test_size=0.2, random_state=42, stratify=y_anomaly
    )
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    # è®­ç»ƒæ¨¡å‹
    model = AnomalyClassifier()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    
    for epoch in range(100):
        model.train()
        total_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch + 1) % 20 == 0:
            model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for batch_x, batch_y in test_loader:
                    output = model(batch_x)
                    _, predicted = torch.max(output.data, 1)
                    total += batch_y.size(0)
                    correct += (predicted == batch_y).sum().item()
            accuracy = 100 * correct / total
            print(f"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Acc: {accuracy:.2f}%")
    
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'anomaly_classifier.pth')
    print("âœ… Anomaly classifier saved as anomaly_classifier.pth")
    return model

def main():
    # ç”Ÿæˆæ•°æ®
    X, y_binary, y_multiclass = generate_data()
    
    # è®­ç»ƒæ£€æµ‹å™¨
    detector = train_detector(X, y_binary)
    
    # è®­ç»ƒåˆ†ç±»å™¨
    classifier = train_classifier(X, y_multiclass)
    
    print("\nğŸ‰ Training complete!")
    print("ğŸ“ Generated files:")
    print("   - anomaly_detector.pth")
    print("   - anomaly_classifier.pth")
    print("   - separate_models_scaler.pkl")

if __name__ == "__main__":
    main() 