import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import joblib

# --- 1. å®šä¹‰å¤šä»»åŠ¡æ¨¡å‹ ---
class MultiTaskAnomalyModel(nn.Module):
    def __init__(self):
        super(MultiTaskAnomalyModel, self).__init__()
        
        # å…±äº«ä¸»å¹²ç½‘ç»œ
        self.shared_layers = nn.Sequential(
            nn.Linear(11, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # è¾“å‡ºå¤´1: å¼‚å¸¸æ£€æµ‹ (2ä¸ªè¾“å‡º: å¼‚å¸¸, æ­£å¸¸)
        self.detection_head = nn.Linear(64, 2)
        
        # è¾“å‡ºå¤´2: å¼‚å¸¸åˆ†ç±» (6ä¸ªè¾“å‡º: 6ç§å¼‚å¸¸ç±»å‹)
        self.classification_head = nn.Linear(64, 6)

    def forward(self, x):
        # æ•°æ®é€šè¿‡å…±äº«å±‚
        features = self.shared_layers(x)
        
        # ç‰¹å¾åˆ†åˆ«è¿›å…¥ä¸¤ä¸ªè¾“å‡ºå¤´
        detection_output = self.detection_head(features)
        classification_output = self.classification_head(features)
        
        # å°†ä¸¤ä¸ªè¾“å‡ºåˆå¹¶æˆä¸€ä¸ªå¼ é‡
        combined_output = torch.cat((detection_output, classification_output), dim=1)
        
        return combined_output

# --- 2. æ•°æ®å‡†å¤‡ ---
def generate_and_prepare_data():
    print("ğŸ”„ Generating and preparing data...")
    # ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„çœŸå®æ•°æ®ç”Ÿæˆé€»è¾‘
    from train_realistic_end_to_end_networks import generate_realistic_network_data
    
    X, y_binary, y_multiclass = generate_realistic_network_data(n_samples=30000)
    
    # æ ‡å‡†åŒ–11ç»´åŸå§‹æ•°æ®
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, 'multitask_scaler.pkl')
    print("âœ… Scaler saved to multitask_scaler.pkl")

    # åˆ†å‰²æ•°æ®é›†
    X_train, X_test, y_binary_train, y_binary_test, y_multiclass_train, y_multiclass_test = train_test_split(
        X_scaled, y_binary, y_multiclass, test_size=0.2, random_state=42, stratify=y_binary
    )
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train)
    X_test_tensor = torch.FloatTensor(X_test)
    y_det_train = torch.LongTensor(y_binary_train)
    y_det_test = torch.LongTensor(y_binary_test)
    y_cls_train = torch.LongTensor(y_multiclass_train)
    y_cls_test = torch.LongTensor(y_multiclass_test)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_det_train, y_cls_train)
    test_dataset = TensorDataset(X_test_tensor, y_det_test, y_cls_test)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    return train_loader, test_loader

# --- 3. è®­ç»ƒé€»è¾‘ ---
def train_model(model, train_loader, test_loader, epochs=120, lr=0.0005): # é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ epoch
    print("ğŸš€ Starting model training (with data augmentation and stronger regularization)...")
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    detection_criterion = nn.CrossEntropyLoss()
    # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬åªå…³å¿ƒå¼‚å¸¸æ ·æœ¬çš„æŸå¤±
    # `ignore_index=0` ä¼šè®©æŸå¤±å‡½æ•°å¿½ç•¥æ‰€æœ‰æ ‡ç­¾ä¸º0ï¼ˆå³â€œæ­£å¸¸â€ï¼‰çš„æ ·æœ¬
    classification_criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4) # å¢åŠ æƒé‡è¡°å‡
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        for batch_x, batch_det_y, batch_cls_y in train_loader:
            
            # --- æ•°æ®å¢å¼ºï¼šåœ¨è®­ç»ƒæ—¶æ·»åŠ å°‘é‡å™ªå£° ---
            noise = torch.randn_like(batch_x) * 0.05 # 5%çš„å™ªå£°
            batch_x_augmented = batch_x + noise

            optimizer.zero_grad()
            
            # è·å–æ¨¡å‹è¾“å‡º
            combined_output = model(batch_x_augmented)
            
            # ä»åˆå¹¶çš„è¾“å‡ºä¸­åˆ†ç¦»å‡ºä¸¤ä¸ªä»»åŠ¡çš„ç»“æœ
            detection_output = combined_output[:, :2]
            classification_output = combined_output[:, 2:]

            # è®¡ç®—ä¸¤ä¸ªä»»åŠ¡çš„æŸå¤±
            loss_det = detection_criterion(detection_output, batch_det_y)

            # è®¡ç®—åˆ†ç±»ä»»åŠ¡çš„æŸå¤± (åªå¯¹å¼‚å¸¸æ ·æœ¬)
            anomaly_mask = (batch_det_y == 1)
            if anomaly_mask.any():
                # æ³¨æ„ï¼šåˆ†ç±»å¤´çš„ç›®æ ‡æ˜¯0-5ï¼Œæ‰€ä»¥åŸå§‹æ ‡ç­¾ï¼ˆ1-6ï¼‰éœ€è¦å‡1
                loss_cls = classification_criterion(
                    classification_output[anomaly_mask], 
                    batch_cls_y[anomaly_mask] - 1
                )
            else:
                # å¦‚æœè¿™ä¸ªæ‰¹æ¬¡é‡Œæ²¡æœ‰å¼‚å¸¸æ ·æœ¬ï¼Œåˆ™åˆ†ç±»æŸå¤±ä¸º0
                loss_cls = torch.tensor(0.0, device=batch_x.device)

            # å°†ä¸¤ä¸ªæŸå¤±åŠ æƒç›¸åŠ 
            # åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæ£€æµ‹ä»»åŠ¡æ›´åŸºç¡€ï¼Œæ‰€ä»¥ç»™å®ƒç¨é«˜çš„æƒé‡
            loss = 0.6 * loss_det + 0.4 * loss_cls
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        avg_loss = total_loss / len(train_loader)
        
        # åœ¨æ¯ä¸ªepochåè¿›è¡Œè¯„ä¼°
        if (epoch + 1) % 10 == 0:
            det_acc, cls_acc = evaluate_model(model, test_loader)
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, "
                  f"Detection Acc: {det_acc:.2f}%, Classification Acc: {cls_acc:.2f}%")

# --- 4. è¯„ä¼°é€»è¾‘ ---
def evaluate_model(model, test_loader):
    model.eval()
    
    correct_det = 0
    total_det = 0
    correct_cls = 0
    total_cls = 0
    
    with torch.no_grad():
        for batch_x, batch_det_y, batch_cls_y in test_loader:
            combined_output = model(batch_x)
            
            # ä»åˆå¹¶çš„è¾“å‡ºä¸­åˆ†ç¦»
            detection_output = combined_output[:, :2]
            classification_output = combined_output[:, 2:]

            # è¯„ä¼°æ£€æµ‹ä»»åŠ¡
            _, predicted_det = torch.max(detection_output.data, 1)
            total_det += batch_det_y.size(0)
            correct_det += (predicted_det == batch_det_y).sum().item()
            
            # è¯„ä¼°åˆ†ç±»ä»»åŠ¡ (åªåœ¨å¼‚å¸¸æ ·æœ¬ä¸Š)
            anomaly_mask = (batch_det_y == 1)
            if anomaly_mask.any():
                _, predicted_cls = torch.max(classification_output[anomaly_mask].data, 1)
                # åˆ†ç±»æ ‡ç­¾éœ€è¦å‡1
                true_cls_labels = batch_cls_y[anomaly_mask] - 1
                total_cls += true_cls_labels.size(0)
                correct_cls += (predicted_cls == true_cls_labels).sum().item()

    det_accuracy = 100 * correct_det / total_det if total_det > 0 else 0
    cls_accuracy = 100 * correct_cls / total_cls if total_cls > 0 else 0
    
    return det_accuracy, cls_accuracy

# --- 5. ä¸»å‡½æ•° ---
def main():
    # å‡†å¤‡æ•°æ®
    train_loader, test_loader = generate_and_prepare_data()
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = MultiTaskAnomalyModel()
    
    # è®­ç»ƒæ¨¡å‹
    train_model(model, train_loader, test_loader)
    
    # ä¿å­˜æ¨¡å‹ä¸ºONNXæ ¼å¼
    print("\nğŸ’¾ Saving model to ONNX format...")
    
    # éœ€è¦ä¸€ä¸ªè™šæ‹Ÿè¾“å…¥æ¥å¯¼å‡º
    dummy_input = torch.randn(1, 11)
    
    # æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºèŠ‚ç‚¹çš„åç§°
    input_names = ["input"]
    output_names = ["combined_output"] # ç°åœ¨åªæœ‰ä¸€ä¸ªè¾“å‡º
    
    torch.onnx.export(model, 
                      (dummy_input,),
                      "multitask_model.onnx",
                      input_names=input_names,
                      output_names=output_names,
                      opset_version=11,
                      dynamic_axes={'input': {0: 'batch_size'},
                                    'combined_output': {0: 'batch_size'}})
    
    print("âœ… Model saved as multitask_model.onnx")
    print("\nğŸ‰ Training complete!")
    print("Next steps:")
    print("1. Use `snpe-onnx-to-dlc` to convert multitask_model.onnx to a single DLC file.")
    print("2. Update the C++ application to use the new single DLC model.")

if __name__ == "__main__":
    main() 