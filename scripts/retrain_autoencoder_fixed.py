#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é‡æ–°è®­ç»ƒè‡ªç¼–ç å™¨æ¨¡å‹
è§£å†³å½“å‰æ¨¡å‹è¾“å‡ºç›¸åŒé‡æ„è¯¯å·®çš„é—®é¢˜
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import os
import json
import joblib
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import logging

def create_improved_autoencoder(input_dim=6, encoding_dim=4):
    """åˆ›å»ºæ”¹è¿›çš„æ·±åº¦è‡ªç¼–ç å™¨"""
    print(f"åˆ›å»ºè‡ªç¼–ç å™¨: è¾“å…¥ç»´åº¦={input_dim}, ç¼–ç ç»´åº¦={encoding_dim}")
    
    # è¾“å…¥å±‚
    input_layer = Input(shape=(input_dim,), name='input')
    
    # ç¼–ç å™¨ (6 -> 8 -> 4)
    encoded = Dense(8, activation='relu', name='encoder_1')(input_layer)
    encoded = BatchNormalization(name='encoder_bn1')(encoded)
    encoded = Dropout(0.1, name='encoder_dropout1')(encoded)
    
    encoded = Dense(encoding_dim, activation='relu', name='encoder_2')(encoded)
    encoded = BatchNormalization(name='encoder_bn2')(encoded)
    
    # è§£ç å™¨ (4 -> 8 -> 6)
    decoded = Dense(8, activation='relu', name='decoder_1')(encoded)
    decoded = BatchNormalization(name='decoder_bn1')(decoded)
    decoded = Dropout(0.1, name='decoder_dropout1')(decoded)
    
    decoded = Dense(input_dim, activation='linear', name='decoder_output')(decoded)
    
    # åˆ›å»ºè‡ªç¼–ç å™¨æ¨¡å‹
    autoencoder = Model(input_layer, decoded, name='autoencoder')
    
    # ç¼–è¯‘æ¨¡å‹
    autoencoder.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return autoencoder

def load_and_prepare_data():
    """åŠ è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv('data/improved_training_data_6d.csv')
    print(f"æ€»æ•°æ®é‡: {len(df)}")
    
    # åªä½¿ç”¨æ­£å¸¸æ•°æ®è¿›è¡Œæ— ç›‘ç£è®­ç»ƒ
    normal_data = df[df['label'] == 0]
    print(f"æ­£å¸¸æ•°æ®é‡: {len(normal_data)}")
    
    # æå–6ç»´ç‰¹å¾
    feature_columns = [
        'avg_signal_strength', 'avg_data_rate', 'avg_latency',
        'packet_loss_rate', 'system_load', 'network_stability'
    ]
    
    X_normal = normal_data[feature_columns].values
    print(f"ç‰¹å¾å½¢çŠ¶: {X_normal.shape}")
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    print("\\næ•°æ®è´¨é‡æ£€æŸ¥:")
    print(f"ç‰¹å¾ç»Ÿè®¡:")
    for i, col in enumerate(feature_columns):
        values = X_normal[:, i]
        print(f"  {col}: [{values.min():.3f}, {values.max():.3f}], std={values.std():.3f}")
    
    # æ•°æ®æ ‡å‡†åŒ–
    print("\\nè¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X_normal)
    
    print("æ ‡å‡†åŒ–åçš„æ•°æ®ç»Ÿè®¡:")
    for i, col in enumerate(feature_columns):
        values = X_scaled[:, i]
        print(f"  {col}: [{values.min():.3f}, {values.max():.3f}], std={values.std():.3f}")
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)
    print(f"\\nè®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"éªŒè¯é›†å¤§å°: {X_val.shape}")
    
    return X_train, X_val, scaler, feature_columns

def train_autoencoder(X_train, X_val):
    """è®­ç»ƒè‡ªç¼–ç å™¨"""
    print("\\nå¼€å§‹è®­ç»ƒè‡ªç¼–ç å™¨...")
    
    # åˆ›å»ºæ¨¡å‹
    autoencoder = create_improved_autoencoder()
    
    # æ‰“å°æ¨¡å‹æ¶æ„
    print("\\næ¨¡å‹æ¶æ„:")
    autoencoder.summary()
    
    # è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            verbose=1
        )
    ]
    
    # è®­ç»ƒæ¨¡å‹
    history = autoencoder.fit(
        X_train, X_train,  # è‡ªç¼–ç å™¨çš„ç›®æ ‡æ˜¯é‡æ„è¾“å…¥
        epochs=200,
        batch_size=64,
        validation_data=(X_val, X_val),
        callbacks=callbacks,
        verbose=1
    )
    
    return autoencoder, history

def evaluate_model(autoencoder, X_train, X_val, scaler):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("\\n=== æ¨¡å‹è¯„ä¼° ===")
    
    # åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°
    train_pred = autoencoder.predict(X_train)
    train_mse = mean_squared_error(X_train, train_pred)
    train_errors = np.mean(np.square(X_train - train_pred), axis=1)
    
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    val_pred = autoencoder.predict(X_val)
    val_mse = mean_squared_error(X_val, val_pred)
    val_errors = np.mean(np.square(X_val - val_pred), axis=1)
    
    print(f"è®­ç»ƒé›† MSE: {train_mse:.6f}")
    print(f"éªŒè¯é›† MSE: {val_mse:.6f}")
    print(f"è®­ç»ƒé›†é‡æ„è¯¯å·®èŒƒå›´: [{train_errors.min():.6f}, {train_errors.max():.6f}]")
    print(f"éªŒè¯é›†é‡æ„è¯¯å·®èŒƒå›´: [{val_errors.min():.6f}, {val_errors.max():.6f}]")
    
    # è®¡ç®—å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆä½¿ç”¨95%åˆ†ä½æ•°ï¼‰
    threshold_95 = np.percentile(train_errors, 95)
    threshold_99 = np.percentile(train_errors, 99)
    
    print(f"\\nå»ºè®®çš„å¼‚å¸¸æ£€æµ‹é˜ˆå€¼:")
    print(f"  95%åˆ†ä½æ•°: {threshold_95:.6f}")
    print(f"  99%åˆ†ä½æ•°: {threshold_99:.6f}")
    
    # æµ‹è¯•å‡ ä¸ªä¸åŒçš„è¾“å…¥
    print("\\n=== å·®å¼‚åŒ–æµ‹è¯• ===")
    test_cases = {
        "æ­£å¸¸1": np.array([[75.0, 1.5, 11.75, 0.005, 12.0, 35.0]]),
        "æ­£å¸¸2": np.array([[70.0, 2.0, 15.0, 0.01, 20.0, 50.0]]),
        "å¼‚å¸¸1": np.array([[25.0, 0.15, 150.0, 0.15, 85.0, 90.0]]),
        "å¼‚å¸¸2": np.array([[10.0, 0.1, 200.0, 0.3, 95.0, 95.0]])
    }
    
    for name, test_data in test_cases.items():
        # æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®
        test_scaled = scaler.transform(test_data)
        
        # é¢„æµ‹
        pred = autoencoder.predict(test_scaled, verbose=0)
        error = np.mean(np.square(test_scaled - pred))
        
        print(f"  {name}: é‡æ„è¯¯å·® = {error:.6f}, å¼‚å¸¸={error > threshold_95}")
    
    return threshold_95

def save_model(autoencoder, scaler, threshold):
    """ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶"""
    print("\\n=== ä¿å­˜æ¨¡å‹ ===")
    
    model_dir = "models/autoencoder_model_retrained"
    os.makedirs(model_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºSavedModelæ ¼å¼
    autoencoder.export(model_dir)
    print(f"âœ… è‡ªç¼–ç å™¨å·²ä¿å­˜åˆ°: {model_dir}")
    
    # ä¿å­˜scaler
    scaler_path = os.path.join(model_dir, 'autoencoder_scaler.pkl')
    joblib.dump(scaler, scaler_path)
    print(f"âœ… Scalerå·²ä¿å­˜åˆ°: {scaler_path}")
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config = {
        'model_type': 'improved_autoencoder',
        'input_features': 6,
        'encoding_dim': 4,
        'threshold': float(threshold),
        'architecture': '6->8->4->8->6',
        'training_samples': 'normal_data_only',
        'scaler_type': 'RobustScaler'
    }
    
    config_path = os.path.join(model_dir, 'model_config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    return model_dir

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ é‡æ–°è®­ç»ƒè‡ªç¼–ç å™¨")
    print("=" * 50)
    
    try:
        # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
        X_train, X_val, scaler, feature_columns = load_and_prepare_data()
        
        # 2. è®­ç»ƒæ¨¡å‹
        autoencoder, history = train_autoencoder(X_train, X_val)
        
        # 3. è¯„ä¼°æ¨¡å‹
        threshold = evaluate_model(autoencoder, X_train, X_val, scaler)
        
        # 4. ä¿å­˜æ¨¡å‹
        model_dir = save_model(autoencoder, scaler, threshold)
        
        print("\\n" + "=" * 50)
        print("ğŸ‰ è‡ªç¼–ç å™¨é‡è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {model_dir}")
        print(f"ğŸ¯ å»ºè®®é˜ˆå€¼: {threshold:.6f}")
        print("\\nä¸‹ä¸€æ­¥: æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„å’Œé˜ˆå€¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 