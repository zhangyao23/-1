#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AIå¼‚å¸¸æ£€æµ‹è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ (ä½¿ç”¨FeatureExtractor)

ä½¿ç”¨FeatureExtractorå°†11ä¸ªåŸå§‹ç½‘ç»œæŒ‡æ ‡è½¬æ¢ä¸º6ä¸ªå·¥ç¨‹ç‰¹å¾ï¼Œ
ç„¶åä½¿ç”¨è¿™äº›å·¥ç¨‹ç‰¹å¾æ¥è®­ç»ƒAIæ¨¡å‹ã€‚

æ­¥éª¤ï¼š
1. ç”Ÿæˆ11ä¸ªåŸå§‹ç½‘ç»œæŒ‡æ ‡çš„æ¨¡æ‹Ÿæ•°æ®
2. ä½¿ç”¨FeatureExtractorè½¬æ¢ä¸º6ç»´ç‰¹å¾å‘é‡
3. ç”Ÿæˆæ ‡å‡†åŒ–çš„6ç»´ç‰¹å¾å‘é‡
4. ä¿å­˜ä¸ºè®­ç»ƒæ•°æ®

ä¸»è¦æ­¥éª¤ï¼š
1. è¯»å–åŸºäºçœŸå®æŒ‡æ ‡çš„åŸå§‹æ•°æ®
2. ä½¿ç”¨FeatureExtractorè¿›è¡Œç‰¹å¾å·¥ç¨‹
3. ç”Ÿæˆæ ‡å‡†åŒ–çš„6ç»´ç‰¹å¾å‘é‡
4. ä¿å­˜ç”¨äºæ¨¡å‹è®­ç»ƒçš„ç‰¹å¾æ•°æ®
"""

import os
import sys
import csv
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.feature_processor.feature_extractor import FeatureExtractor
from src.logger.system_logger import SystemLogger

# æ•°æ®ç›®å½•
DATA_DIR = os.path.join(project_root, 'data')
REALISTIC_NORMAL_FILE = os.path.join(DATA_DIR, 'realistic_normal_traffic.csv')
REALISTIC_ANOMALIES_FILE = os.path.join(DATA_DIR, 'realistic_labeled_anomalies.csv')

# è¾“å‡ºæ–‡ä»¶
PROCESSED_NORMAL_FILE = os.path.join(DATA_DIR, 'processed_normal_traffic.csv')
PROCESSED_ANOMALIES_FILE = os.path.join(DATA_DIR, 'processed_labeled_anomalies.csv')

# é…ç½®æ–‡ä»¶
CONFIG_FILE = os.path.join(project_root, 'config', 'system_config.json')

def load_config():
    """åŠ è½½ç³»ç»Ÿé…ç½®"""
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {CONFIG_FILE}: {e}")
        # ä½¿ç”¨é»˜è®¤é…ç½®
        return {
            'logging': {
                'level': 'INFO',
                'file': 'logs/training.log'
            },
            'metrics': [
                'wlan0_wireless_quality', 'wlan0_signal_level', 'wlan0_noise_level',
                'wlan0_rx_packets', 'wlan0_tx_packets', 'wlan0_rx_bytes', 'wlan0_tx_bytes',
                'gateway_ping_time', 'dns_resolution_time', 'memory_usage_percent', 'cpu_usage_percent'
            ]
        }

def convert_to_dict_format(row_data: Dict[str, str]) -> Dict[str, float]:
    """å°†CSVè¡Œæ•°æ®è½¬æ¢ä¸ºç‰¹å¾æå–å™¨éœ€è¦çš„å­—å…¸æ ¼å¼"""
    converted = {}
    for key, value in row_data.items():
        if key != 'label':  # è·³è¿‡æ ‡ç­¾åˆ—
            try:
                converted[key] = float(value)
            except (ValueError, TypeError):
                converted[key] = 0.0
    return converted

def process_normal_data(feature_extractor: FeatureExtractor, logger: SystemLogger):
    """å¤„ç†æ­£å¸¸æ•°æ®ï¼Œè½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
    print("ğŸ”„ å¤„ç†æ­£å¸¸æ•°æ®...")
    
    if not os.path.exists(REALISTIC_NORMAL_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°åŸå§‹æ­£å¸¸æ•°æ®æ–‡ä»¶ {REALISTIC_NORMAL_FILE}")
        print("è¯·å…ˆè¿è¡Œï¼špython scripts/generate_realistic_training_data.py")
        return None
    
    # è¯»å–åŸå§‹æ•°æ®
    normal_data = pd.read_csv(REALISTIC_NORMAL_FILE)
    print(f"   åŸå§‹æ•°æ®: {len(normal_data)} æ¡è®°å½•ï¼Œ{len(normal_data.columns)} ä¸ªæŒ‡æ ‡")
    
    processed_features = []
    feature_names = None
    
    for index, row in normal_data.iterrows():
        if (index + 1) % 1000 == 0:
            print(f"   è¿›åº¦: {index + 1}/{len(normal_data)}")
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        row_dict = convert_to_dict_format(row.to_dict())
        
        # æå–ç‰¹å¾
        feature_vector = feature_extractor.extract_features(row_dict)
        
        if len(feature_vector) > 0:
            processed_features.append(feature_vector)
            
            # è·å–ç‰¹å¾åç§°ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
            if feature_names is None:
                feature_names = feature_extractor.get_feature_names()
    
    if not processed_features:
        print("âŒ é”™è¯¯ï¼šæœªèƒ½æå–ä»»ä½•ç‰¹å¾")
        return None
    
    # è½¬æ¢ä¸ºæ•°ç»„
    processed_features = np.array(processed_features)
    print(f"   ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(processed_features)} æ¡è®°å½•ï¼Œ{processed_features.shape[1]} ä¸ªç‰¹å¾")
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    feature_df = pd.DataFrame(processed_features, columns=feature_names)
    feature_df.to_csv(PROCESSED_NORMAL_FILE, index=False)
    
    print(f"âœ… æ­£å¸¸æ•°æ®ç‰¹å¾å·²ä¿å­˜åˆ°: {PROCESSED_NORMAL_FILE}")
    return processed_features, feature_names

def process_anomaly_data(feature_extractor: FeatureExtractor, logger: SystemLogger):
    """å¤„ç†å¼‚å¸¸æ•°æ®ï¼Œè½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
    print("ğŸ”„ å¤„ç†å¼‚å¸¸æ•°æ®...")
    
    if not os.path.exists(REALISTIC_ANOMALIES_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°åŸå§‹å¼‚å¸¸æ•°æ®æ–‡ä»¶ {REALISTIC_ANOMALIES_FILE}")
        print("è¯·å…ˆè¿è¡Œï¼špython scripts/generate_realistic_training_data.py")
        return None
    
    # è¯»å–åŸå§‹æ•°æ®
    anomaly_data = pd.read_csv(REALISTIC_ANOMALIES_FILE)
    print(f"   åŸå§‹æ•°æ®: {len(anomaly_data)} æ¡è®°å½•")
    
    processed_features = []
    labels = []
    feature_names = None
    
    for index, row in anomaly_data.iterrows():
        if (index + 1) % 200 == 0:
            print(f"   è¿›åº¦: {index + 1}/{len(anomaly_data)}")
        
        # æå–æ ‡ç­¾
        label = row.get('label', 'unknown')
        labels.append(label)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆæ’é™¤æ ‡ç­¾åˆ—ï¼‰
        row_dict = convert_to_dict_format(row.to_dict())
        
        # æå–ç‰¹å¾
        feature_vector = feature_extractor.extract_features(row_dict)
        
        if len(feature_vector) > 0:
            processed_features.append(feature_vector)
            
            # è·å–ç‰¹å¾åç§°ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
            if feature_names is None:
                feature_names = feature_extractor.get_feature_names()
        else:
            # å¦‚æœç‰¹å¾æå–å¤±è´¥ï¼Œç§»é™¤å¯¹åº”çš„æ ‡ç­¾
            labels.pop()
    
    if not processed_features:
        print("âŒ é”™è¯¯ï¼šæœªèƒ½æå–ä»»ä½•ç‰¹å¾")
        return None
    
    # è½¬æ¢ä¸ºæ•°ç»„
    processed_features = np.array(processed_features)
    print(f"   ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(processed_features)} æ¡è®°å½•ï¼Œ{processed_features.shape[1]} ä¸ªç‰¹å¾")
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    feature_df = pd.DataFrame(processed_features, columns=feature_names)
    feature_df['label'] = labels
    feature_df.to_csv(PROCESSED_ANOMALIES_FILE, index=False)
    
    print(f"âœ… å¼‚å¸¸æ•°æ®ç‰¹å¾å·²ä¿å­˜åˆ°: {PROCESSED_ANOMALIES_FILE}")
    
    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    label_counts = pd.Series(labels).value_counts()
    print("   å¼‚å¸¸ç±»åˆ«åˆ†å¸ƒ:")
    for label, count in label_counts.items():
        print(f"     {label}: {count} æ¡")
    
    return processed_features, labels, feature_names

def analyze_features(feature_names: List[str]):
    """åˆ†æç”Ÿæˆçš„ç‰¹å¾"""
    print("\nğŸ“Š ç‰¹å¾åˆ†æ:")
    print(f"âœ… æ€»ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print("âœ… ç‰¹å¾åˆ—è¡¨:")
    
    # æŒ‰ç±»åˆ«åˆ†ç»„æ˜¾ç¤ºç‰¹å¾
    basic_features = [f for f in feature_names if not any(suffix in f for suffix in ['_trend', '_volatility', '_momentum', '_change_rate', '_mean', '_std', '_median', '_range'])]
    statistical_features = [f for f in feature_names if any(suffix in f for suffix in ['_mean', '_std', '_median', '_range', 'global_'])]
    temporal_features = [f for f in feature_names if any(suffix in f for suffix in ['_trend', '_volatility', '_momentum', '_change_rate'])]
    
    print(f"   åŸºç¡€ç‰¹å¾ ({len(basic_features)}): {basic_features[:5]}..." if len(basic_features) > 5 else f"   åŸºç¡€ç‰¹å¾: {basic_features}")
    print(f"   ç»Ÿè®¡ç‰¹å¾ ({len(statistical_features)}): {statistical_features[:3]}..." if len(statistical_features) > 3 else f"   ç»Ÿè®¡ç‰¹å¾: {statistical_features}")
    print(f"   æ—¶é—´ç‰¹å¾ ({len(temporal_features)}): {temporal_features[:3]}..." if len(temporal_features) > 3 else f"   æ—¶é—´ç‰¹å¾: {temporal_features}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŸºäºç‰¹å¾å·¥ç¨‹çš„è®­ç»ƒæ•°æ®ç”Ÿæˆ")
    print("="*60)
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = SystemLogger(config['logging'])
    
    # åˆ›å»ºç‰¹å¾æå–å™¨ï¼ˆä¸ä½¿ç”¨é¢„è®­ç»ƒçš„scalerï¼Œç”¨äºè®­ç»ƒæ¨¡å¼ï¼‰
    print("ğŸ”§ åˆå§‹åŒ–ç‰¹å¾æå–å™¨...")
    # ä½¿ç”¨11ä¸ªçœŸå®ç½‘ç»œæŒ‡æ ‡é…ç½®
    real_metrics = [
        'wlan0_wireless_quality', 'wlan0_signal_level', 'wlan0_noise_level',
        'wlan0_rx_packets', 'wlan0_tx_packets', 'wlan0_rx_bytes', 'wlan0_tx_bytes',
        'gateway_ping_time', 'dns_resolution_time', 'memory_usage_percent', 'cpu_usage_percent'
    ]
    feature_extractor = FeatureExtractor(
        metrics_config=real_metrics,
        logger=logger,
        scaler_path=None  # ä¸ä½¿ç”¨é¢„è®­ç»ƒscaler
    )
    
    # å¤„ç†æ­£å¸¸æ•°æ®
    normal_result = process_normal_data(feature_extractor, logger)
    if normal_result is None:
        print("âŒ æ­£å¸¸æ•°æ®å¤„ç†å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    normal_features, feature_names = normal_result
    
    # å¤„ç†å¼‚å¸¸æ•°æ®
    anomaly_result = process_anomaly_data(feature_extractor, logger)
    if anomaly_result is None:
        print("âŒ å¼‚å¸¸æ•°æ®å¤„ç†å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    anomaly_features, anomaly_labels, _ = anomaly_result
    
    # åˆ†æç‰¹å¾
    analyze_features(feature_names)
    
    print("\nğŸ¯ ç‰¹å¾å·¥ç¨‹æ•°æ®ç”Ÿæˆå®Œæˆ!")
    print("æ–°æ•°æ®ç‰¹ç‚¹:")
    print("âœ… ä½¿ç”¨æ ‡å‡†ç‰¹å¾æå–å™¨è¿›è¡Œç‰¹å¾å·¥ç¨‹")
    print("âœ… ä»11ä¸ªåŸå§‹æŒ‡æ ‡ç”Ÿæˆ6ä¸ªå·¥ç¨‹ç‰¹å¾")
    print("âœ… ç‰¹å¾æ ¼å¼ä¸å®é™…æµ‹è¯•ç¯å¢ƒå®Œå…¨ä¸€è‡´")
    print("âœ… åŒ…å«åŸºç¡€ã€ç»Ÿè®¡å’Œæ—¶é—´åºåˆ—ç‰¹å¾")
    
    print(f"\nğŸ“‚ è¾“å‡ºæ–‡ä»¶:")
    print(f"   æ­£å¸¸æ•°æ®: {PROCESSED_NORMAL_FILE}")
    print(f"   å¼‚å¸¸æ•°æ®: {PROCESSED_ANOMALIES_FILE}")
    
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. ä½¿ç”¨å¤„ç†åçš„ç‰¹å¾æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹:")
    print(f"   python scripts/train_model.py autoencoder --data_path {PROCESSED_NORMAL_FILE}")
    print(f"   python scripts/train_model.py classifier --data_path {PROCESSED_ANOMALIES_FILE}")
    print("2. éªŒè¯æ¨¡å‹åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°")

if __name__ == "__main__":
    main() 