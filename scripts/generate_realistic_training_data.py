#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºçœŸå®ç½‘ç»œæŒ‡æ ‡çš„è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨

æ ¹æ®å®é™…çš„11ä¸ªç½‘ç»œæŒ‡æ ‡ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œç¡®ä¿æ¨¡å‹è®­ç»ƒä¸å®é™…æµ‹è¯•æ•°æ®æ ¼å¼ä¸€è‡´ï¼š
1. åŸºäºçœŸå®ç½‘ç»œæŒ‡æ ‡çš„æ­£å¸¸æ•°æ®åˆ†å¸ƒ
2. æ¨¡æ‹Ÿå„ç§ç½‘ç»œå¼‚å¸¸åœºæ™¯çš„å¼‚å¸¸æ•°æ®
3. ç¡®ä¿æ•°æ®åˆ†å¸ƒæ›´æ¥è¿‘å®é™…ç½‘ç»œç¯å¢ƒ
"""

import os
import csv
import json
import numpy as np
import random
from typing import Dict, List, Any

# æ•°æ®ç›®å½•
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')
REALISTIC_NORMAL_FILE = os.path.join(DATA_DIR, 'realistic_normal_traffic.csv')
REALISTIC_ANOMALIES_FILE = os.path.join(DATA_DIR, 'realistic_labeled_anomalies.csv')

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)

# çœŸå®ç½‘ç»œæŒ‡æ ‡çš„å®šä¹‰
REAL_NETWORK_METRICS = [
    'wlan0_wireless_quality',      # WiFiä¿¡å·è´¨é‡ (0-100)
    'wlan0_signal_level',          # WiFiä¿¡å·å¼ºåº¦ (é€šå¸¸è´Ÿå€¼dBm)
    'wlan0_noise_level',           # WiFiå™ªå£°æ°´å¹³ (é€šå¸¸è´Ÿå€¼dBm)
    'wlan0_rx_packets',            # æ¥æ”¶æ•°æ®åŒ…æ•°é‡
    'wlan0_tx_packets',            # å‘é€æ•°æ®åŒ…æ•°é‡
    'wlan0_rx_bytes',              # æ¥æ”¶å­—èŠ‚æ•°
    'wlan0_tx_bytes',              # å‘é€å­—èŠ‚æ•°
    'gateway_ping_time',           # ç½‘å…³Pingå»¶è¿Ÿ (ms)
    'dns_resolution_time',         # DNSè§£ææ—¶é—´ (ms)
    'memory_usage_percent',        # å†…å­˜ä½¿ç”¨ç‡ (0-100%)
    'cpu_usage_percent'            # CPUä½¿ç”¨ç‡ (0-100%)
]

# æ­£å¸¸ç½‘ç»œç¯å¢ƒçš„å…¸å‹å€¼èŒƒå›´
NORMAL_RANGES = {
    'wlan0_wireless_quality': (60, 100),        # è‰¯å¥½çš„WiFiè´¨é‡
    'wlan0_signal_level': (-65, -30),           # è‰¯å¥½çš„ä¿¡å·å¼ºåº¦
    'wlan0_noise_level': (-90, -70),            # ä½å™ªå£°
    'wlan0_rx_packets': (50, 200),              # æ­£å¸¸åŒ…æ•°é‡
    'wlan0_tx_packets': (30, 150),              # æ­£å¸¸åŒ…æ•°é‡
    'wlan0_rx_bytes': (1024, 10240),            # æ­£å¸¸å­—èŠ‚æ•°
    'wlan0_tx_bytes': (512, 8192),              # æ­£å¸¸å­—èŠ‚æ•°
    'gateway_ping_time': (1, 20),               # è‰¯å¥½çš„å»¶è¿Ÿ
    'dns_resolution_time': (5, 50),             # æ­£å¸¸DNSå“åº”
    'memory_usage_percent': (20, 70),           # æ­£å¸¸å†…å­˜ä½¿ç”¨
    'cpu_usage_percent': (5, 40)                # æ­£å¸¸CPUä½¿ç”¨
}

# å¼‚å¸¸åœºæ™¯å®šä¹‰
ANOMALY_SCENARIOS = {
    'signal_degradation': {
        'description': 'WiFiä¿¡å·è¡°å‡',
        'modifications': {
            'wlan0_wireless_quality': (0, 40),
            'wlan0_signal_level': (-90, -70),
            'gateway_ping_time': (50, 200)
        }
    },
    'network_congestion': {
        'description': 'ç½‘ç»œæ‹¥å¡',
        'modifications': {
            'gateway_ping_time': (100, 500),
            'dns_resolution_time': (100, 300),
            'wlan0_rx_packets': (500, 2000),
            'wlan0_tx_packets': (400, 1800)
        }
    },
    'high_interference': {
        'description': 'é«˜å¹²æ‰°ç¯å¢ƒ',
        'modifications': {
            'wlan0_noise_level': (-60, -40),
            'wlan0_wireless_quality': (10, 50),
            'wlan0_signal_level': (-80, -60)
        }
    },
    'resource_overload': {
        'description': 'ç³»ç»Ÿèµ„æºè¿‡è½½',
        'modifications': {
            'memory_usage_percent': (80, 95),
            'cpu_usage_percent': (70, 95),
            'gateway_ping_time': (30, 100)
        }
    },
    'connection_issues': {
        'description': 'è¿æ¥é—®é¢˜',
        'modifications': {
            'gateway_ping_time': (200, 1000),
            'dns_resolution_time': (200, 800),
            'wlan0_rx_packets': (0, 20),
            'wlan0_tx_packets': (0, 15)
        }
    },
    'bandwidth_saturation': {
        'description': 'å¸¦å®½é¥±å’Œ',
        'modifications': {
            'wlan0_rx_bytes': (50000, 100000),
            'wlan0_tx_bytes': (40000, 90000),
            'wlan0_rx_packets': (1000, 5000),
            'wlan0_tx_packets': (800, 4000)
        }
    },
    'mixed_anomaly': {
        'description': 'æ··åˆå¼‚å¸¸',
        'modifications': {
            'wlan0_wireless_quality': (0, 30),
            'memory_usage_percent': (85, 98),
            'gateway_ping_time': (100, 300),
            'dns_resolution_time': (100, 400)
        }
    }
}

def generate_normal_sample() -> Dict[str, float]:
    """ç”Ÿæˆä¸€ä¸ªæ­£å¸¸ç½‘ç»œçŠ¶æ€çš„æ•°æ®æ ·æœ¬"""
    sample = {}
    
    for metric in REAL_NETWORK_METRICS:
        min_val, max_val = NORMAL_RANGES[metric]
        
        # æ·»åŠ ä¸€äº›ç›¸å…³æ€§æ¨¡æ‹ŸçœŸå®ç½‘ç»œè¡Œä¸º
        if metric == 'wlan0_wireless_quality':
            # WiFiè´¨é‡ä½œä¸ºåŸºç¡€å‚è€ƒ
            base_quality = np.random.normal((min_val + max_val) / 2, (max_val - min_val) / 6)
            sample[metric] = max(min_val, min(max_val, base_quality))
        
        elif metric == 'wlan0_signal_level':
            # ä¿¡å·å¼ºåº¦ä¸è´¨é‡ç›¸å…³
            quality = sample.get('wlan0_wireless_quality', 80)
            # è´¨é‡è¶Šé«˜ï¼Œä¿¡å·å¼ºåº¦è¶Šå¥½ï¼ˆæ•°å€¼è¶Šæ¥è¿‘0ï¼‰
            signal_base = -30 - (100 - quality) * 0.35
            sample[metric] = max(min_val, min(max_val, signal_base + np.random.normal(0, 5)))
        
        elif metric == 'gateway_ping_time':
            # Pingæ—¶é—´ä¸WiFiè´¨é‡è´Ÿç›¸å…³
            quality = sample.get('wlan0_wireless_quality', 80)
            ping_base = max_val - (quality - min_val) / (100 - min_val) * (max_val - min_val)
            sample[metric] = max(min_val, ping_base + np.random.normal(0, 3))
        
        elif metric in ['wlan0_rx_packets', 'wlan0_tx_packets']:
            # æ•°æ®åŒ…æ•°é‡æœ‰ä¸€å®šç›¸å…³æ€§
            base_packets = np.random.normal((min_val + max_val) / 2, (max_val - min_val) / 6)
            sample[metric] = max(min_val, min(max_val, base_packets))
        
        elif metric in ['wlan0_rx_bytes', 'wlan0_tx_bytes']:
            # å­—èŠ‚æ•°ä¸åŒ…æ•°é‡ç›¸å…³
            packets_key = metric.replace('_bytes', '_packets')
            if packets_key in sample:
                avg_packet_size = np.random.uniform(50, 200)  # å¹³å‡åŒ…å¤§å°
                base_bytes = sample[packets_key] * avg_packet_size
                sample[metric] = max(min_val, min(max_val, base_bytes))
            else:
                sample[metric] = np.random.uniform(min_val, max_val)
        
        elif metric in ['memory_usage_percent', 'cpu_usage_percent']:
            # ç³»ç»Ÿèµ„æºä½¿ç”¨æœ‰ä¸€å®šç›¸å…³æ€§
            base_usage = np.random.normal((min_val + max_val) / 2, (max_val - min_val) / 8)
            sample[metric] = max(min_val, min(max_val, base_usage))
        
        else:
            # å…¶ä»–æŒ‡æ ‡ä½¿ç”¨æ­£æ€åˆ†å¸ƒ
            mean = (min_val + max_val) / 2
            std = (max_val - min_val) / 6
            sample[metric] = max(min_val, min(max_val, np.random.normal(mean, std)))
    
    return sample

def generate_anomaly_sample(scenario_name: str) -> Dict[str, Any]:
    """ç”ŸæˆæŒ‡å®šå¼‚å¸¸åœºæ™¯çš„æ•°æ®æ ·æœ¬"""
    if scenario_name not in ANOMALY_SCENARIOS:
        raise ValueError(f"æœªçŸ¥çš„å¼‚å¸¸åœºæ™¯: {scenario_name}")
    
    scenario = ANOMALY_SCENARIOS[scenario_name]
    sample = generate_normal_sample()  # ä»æ­£å¸¸æ ·æœ¬å¼€å§‹
    
    # åº”ç”¨å¼‚å¸¸ä¿®æ”¹
    for metric, (min_val, max_val) in scenario['modifications'].items():
        if metric in sample:
            # åœ¨å¼‚å¸¸èŒƒå›´å†…ç”Ÿæˆå€¼
            sample[metric] = np.random.uniform(min_val, max_val)
    
    # æ·»åŠ æ ‡ç­¾
    sample['label'] = scenario_name
    
    return sample

def generate_realistic_normal_data(num_samples: int = 15000):
    """ç”ŸæˆåŸºäºçœŸå®ç½‘ç»œæŒ‡æ ‡çš„æ­£å¸¸æ•°æ®"""
    print(f"ğŸ”„ ç”Ÿæˆ {num_samples} æ¡åŸºäºçœŸå®ç½‘ç»œæŒ‡æ ‡çš„æ­£å¸¸æ•°æ®...")
    
    normal_samples = []
    for i in range(num_samples):
        if (i + 1) % 1000 == 0:
            print(f"  è¿›åº¦: {i + 1}/{num_samples}")
        
        sample = generate_normal_sample()
        normal_samples.append([sample[metric] for metric in REAL_NETWORK_METRICS])
    
    # ä¿å­˜åˆ°CSVæ–‡ä»¶
    with open(REALISTIC_NORMAL_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(REAL_NETWORK_METRICS)  # ä½¿ç”¨çœŸå®æŒ‡æ ‡åç§°ä½œä¸ºè¡¨å¤´
        writer.writerows(normal_samples)
    
    print(f"âœ… æ­£å¸¸æ•°æ®å·²ä¿å­˜åˆ°: {REALISTIC_NORMAL_FILE}")
    return normal_samples

def generate_realistic_anomaly_data(samples_per_scenario: int = 300):
    """ç”ŸæˆåŸºäºçœŸå®ç½‘ç»œæŒ‡æ ‡çš„å¼‚å¸¸æ•°æ®"""
    print(f"ğŸ”„ ç”Ÿæˆå¼‚å¸¸æ•°æ®ï¼Œæ¯ä¸ªåœºæ™¯ {samples_per_scenario} ä¸ªæ ·æœ¬...")
    
    anomaly_samples = []
    
    for scenario_name, scenario_info in ANOMALY_SCENARIOS.items():
        print(f"  ç”Ÿæˆåœºæ™¯: {scenario_name} - {scenario_info['description']}")
        
        for i in range(samples_per_scenario):
            sample = generate_anomaly_sample(scenario_name)
            row = [sample[metric] for metric in REAL_NETWORK_METRICS] + [sample['label']]
            anomaly_samples.append(row)
    
    # éšæœºæ‰“ä¹±æ•°æ®
    np.random.shuffle(anomaly_samples)
    
    # ä¿å­˜åˆ°CSVæ–‡ä»¶
    with open(REALISTIC_ANOMALIES_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(REAL_NETWORK_METRICS + ['label'])
        writer.writerows(anomaly_samples)
    
    total_anomalies = len(anomaly_samples)
    print(f"âœ… å¼‚å¸¸æ•°æ®å·²ä¿å­˜åˆ°: {REALISTIC_ANOMALIES_FILE}")
    print(f"   æ€»è®¡ {total_anomalies} æ¡å¼‚å¸¸æ•°æ®ï¼Œ{len(ANOMALY_SCENARIOS)} ä¸ªåœºæ™¯")
    
    return anomaly_samples

def analyze_data_distribution():
    """åˆ†æç”Ÿæˆçš„æ•°æ®åˆ†å¸ƒ"""
    print("\nğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ:")
    
    # åˆ†ææ­£å¸¸æ•°æ®
    if os.path.exists(REALISTIC_NORMAL_FILE):
        normal_data = []
        with open(REALISTIC_NORMAL_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                normal_data.append({k: float(v) for k, v in row.items()})
        
        print(f"âœ… æ­£å¸¸æ•°æ®: {len(normal_data)} æ¡")
        
        # æ˜¾ç¤ºæ¯ä¸ªæŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
        for metric in REAL_NETWORK_METRICS[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé¿å…è¾“å‡ºè¿‡å¤š
            values = [sample[metric] for sample in normal_data]
            print(f"   {metric}: å‡å€¼={np.mean(values):.2f}, æ ‡å‡†å·®={np.std(values):.2f}")
    
    # åˆ†æå¼‚å¸¸æ•°æ®
    if os.path.exists(REALISTIC_ANOMALIES_FILE):
        anomaly_data = []
        with open(REALISTIC_ANOMALIES_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                anomaly_data.append(row)
        
        print(f"âœ… å¼‚å¸¸æ•°æ®: {len(anomaly_data)} æ¡")
        
        # æŒ‰åœºæ™¯ç»Ÿè®¡
        scenario_counts = {}
        for sample in anomaly_data:
            label = sample['label']
            scenario_counts[label] = scenario_counts.get(label, 0) + 1
        
        for scenario, count in scenario_counts.items():
            print(f"   {scenario}: {count} æ¡")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç”ŸæˆåŸºäºçœŸå®ç½‘ç»œæŒ‡æ ‡çš„è®­ç»ƒæ•°æ®")
    print("="*60)
    
    # ç”Ÿæˆæ­£å¸¸æ•°æ®
    normal_samples = generate_realistic_normal_data(15000)
    
    # ç”Ÿæˆå¼‚å¸¸æ•°æ®
    anomaly_samples = generate_realistic_anomaly_data(300)
    
    # åˆ†ææ•°æ®åˆ†å¸ƒ
    analyze_data_distribution()
    
    print("\nğŸ¯ æ•°æ®ç”Ÿæˆå®Œæˆ!")
    print("æ–°æ•°æ®ç‰¹ç‚¹:")
    print("âœ… åŸºäº11ä¸ªçœŸå®ç½‘ç»œæŒ‡æ ‡")
    print("âœ… æ­£å¸¸æ•°æ®æ¨¡æ‹ŸçœŸå®ç½‘ç»œç¯å¢ƒ")
    print("âœ… å¼‚å¸¸æ•°æ®è¦†ç›–7ç§å¸¸è§ç½‘ç»œé—®é¢˜")
    print("âœ… æ•°æ®åˆ†å¸ƒæ›´æ¥è¿‘å®é™…ä½¿ç”¨åœºæ™¯")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. ä½¿ç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹:")
    print("   python scripts/train_models.py --data realistic")
    print("2. éªŒè¯æ¨¡å‹åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°")
    
if __name__ == "__main__":
    main() 