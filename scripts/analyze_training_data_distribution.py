#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆ†æè®­ç»ƒæ•°æ®çš„åˆ†å¸ƒç‰¹å¾
ç†è§£è‡ªç¼–ç å™¨å­¦åˆ°äº†ä»€ä¹ˆæ ·çš„"æ­£å¸¸"æ¨¡å¼
"""

import sys
import os
import json
import numpy as np
import pandas as pd
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.feature_processor.feature_extractor import FeatureExtractor

def analyze_training_data():
    """åˆ†æè®­ç»ƒæ•°æ®çš„åˆ†å¸ƒ"""
    try:
        # å…ˆå°è¯•åŠ è½½.npzæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åŠ è½½CSVæ–‡ä»¶
        training_data_path = os.path.join(project_root, 'data', 'training_data_v2.npz')
        
        if os.path.exists(training_data_path):
            # åŠ è½½.npzæ–‡ä»¶
            data = np.load(training_data_path)
            X = data['X']
        else:
            # å°è¯•åŠ è½½CSVæ–‡ä»¶
            csv_path = os.path.join(project_root, 'data', 'enhanced_training_data.csv')
            if os.path.exists(csv_path):
                print(f"ğŸ“ ä½¿ç”¨CSVè®­ç»ƒæ•°æ®: {csv_path}")
                df = pd.read_csv(csv_path)
                
                # å‡è®¾å‰6åˆ—æ˜¯ç‰¹å¾ï¼Œæœ€åä¸€åˆ—æ˜¯æ ‡ç­¾
                if len(df.columns) >= 7:
                    X = df.iloc[:, :6].values
                else:
                    print(f"âŒ CSVæ–‡ä»¶åˆ—æ•°ä¸è¶³: {len(df.columns)}")
                    return None
            else:
                print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
                print(f"  å°è¯•è¿‡çš„è·¯å¾„:")
                print(f"  - {training_data_path}")
                print(f"  - {csv_path}")
                return None
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: {X.shape}")
        print(f"   æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        
        # ç»Ÿè®¡æ¯ä¸ªç‰¹å¾çš„åˆ†å¸ƒ
        print(f"\nğŸ“ˆ è®­ç»ƒæ•°æ®ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡:")
        print("=" * 80)
        print(f"{'ç‰¹å¾':<12} {'æœ€å°å€¼':<12} {'æœ€å¤§å€¼':<12} {'å¹³å‡å€¼':<12} {'æ ‡å‡†å·®':<12} {'èŒƒå›´':<12}")
        print("-" * 80)
        
        feature_stats = []
        for i in range(X.shape[1]):
            feature_data = X[:, i]
            min_val = np.min(feature_data)
            max_val = np.max(feature_data)
            mean_val = np.mean(feature_data)
            std_val = np.std(feature_data)
            range_val = max_val - min_val
            
            feature_stats.append({
                'feature': f'feature_{i:02d}',
                'min': min_val,
                'max': max_val,
                'mean': mean_val,
                'std': std_val,
                'range': range_val
            })
            
            print(f"feature_{i:02d}  {min_val:>11.2f} {max_val:>11.2f} {mean_val:>11.2f} {std_val:>11.2f} {range_val:>11.2f}")
        
        # åˆ›å»ºä¸€äº›æµ‹è¯•ç‚¹æ¥çœ‹é‡æ„è¯¯å·®
        print(f"\nğŸ§ª æµ‹è¯•ä¸åŒç¨‹åº¦çš„åç¦»å¯¹é‡æ„è¯¯å·®çš„å½±å“:")
        print("=" * 80)
        
        # åŠ è½½é…ç½®å’Œæ¨¡å‹
        config_path = os.path.join(project_root, 'config', 'system_config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        from src.ai_models.autoencoder_model import AutoencoderModel
        
        class SimpleLogger:
            def info(self, msg): pass
            def error(self, msg): print(f"ERROR: {msg}")
            def debug(self, msg): pass
            def warning(self, msg): pass
        
        logger = SimpleLogger()
        autoencoder = AutoencoderModel(config['ai_models']['autoencoder'], logger)
        
        # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ä½œä¸ºåŸºå‡†
        baseline = np.mean(X, axis=0)
        baseline_error = autoencoder.predict(baseline)['reconstruction_error']
        print(f"è®­ç»ƒæ•°æ®å‡å€¼çš„é‡æ„è¯¯å·®: {baseline_error:.2f}")
        
        # æµ‹è¯•ä¸åŒç¨‹åº¦çš„åç¦»
        print(f"\nåç¦»ç¨‹åº¦æµ‹è¯•:")
        print(f"{'åç¦»ç±»å‹':<20} {'é‡æ„è¯¯å·®':<12} {'ç›¸å¯¹åŸºå‡†':<12}")
        print("-" * 50)
        
        # 1. è½»å¾®åç¦»ï¼ˆÂ±0.5 stdï¼‰
        for factor in [0.5, 1.0, 2.0, 5.0]:
            test_point = baseline.copy()
            # éšæœºé€‰æ‹©å‡ ä¸ªç‰¹å¾è¿›è¡Œåç¦»
            selected_features = np.random.choice(X.shape[1], size=5, replace=False)
            for feat_idx in selected_features:
                std_val = feature_stats[feat_idx]['std']
                test_point[feat_idx] += factor * std_val
            
            error = autoencoder.predict(test_point)['reconstruction_error']
            relative = error / baseline_error if baseline_error > 0 else 0
            print(f"åç¦»{factor}å€æ ‡å‡†å·®        {error:>11.2f} {relative:>11.2f}x")
        
        # 2. æç«¯å€¼æµ‹è¯•
        print(f"\næç«¯å€¼æµ‹è¯•:")
        print(f"{'æç«¯ç±»å‹':<20} {'é‡æ„è¯¯å·®':<12} {'ç›¸å¯¹åŸºå‡†':<12}")
        print("-" * 50)
        
        # å…¨éƒ¨æœ€å°å€¼
        extreme_min = np.array([stat['min'] for stat in feature_stats])
        error_min = autoencoder.predict(extreme_min)['reconstruction_error']
        relative_min = error_min / baseline_error if baseline_error > 0 else 0
        print(f"å…¨éƒ¨æœ€å°å€¼             {error_min:>11.2f} {relative_min:>11.2f}x")
        
        # å…¨éƒ¨æœ€å¤§å€¼
        extreme_max = np.array([stat['max'] for stat in feature_stats])
        error_max = autoencoder.predict(extreme_max)['reconstruction_error']
        relative_max = error_max / baseline_error if baseline_error > 0 else 0
        print(f"å…¨éƒ¨æœ€å¤§å€¼             {error_max:>11.2f} {relative_max:>11.2f}x")
        
        # 3. åˆ†ææˆ‘ä»¬çš„æµ‹è¯•åœºæ™¯ä¸ºä»€ä¹ˆè¯¯å·®ä½
        print(f"\nğŸ” åˆ†ææµ‹è¯•åœºæ™¯çš„ç‰¹å¾å€¼:")
        print("=" * 80)
        
        # åˆ›å»ºæ­£å¸¸åœºæ™¯
        normal_data = {
            'wlan0_wireless_quality': 85.0,
            'wlan0_signal_level': -45.0,
            'wlan0_noise_level': -90.0,
            'wlan0_rx_packets': 50000,
            'wlan0_tx_packets': 35000,
            'wlan0_rx_bytes': 80000000,
            'wlan0_tx_bytes': 30000000,
            'gateway_ping_time': 8.0,
            'dns_resolution_time': 15.0,
            'memory_usage_percent': 35.0,
            'cpu_usage_percent': 15.0
        }
        
        # åˆ›å»ºä¿¡å·è¡°å‡å¼‚å¸¸åœºæ™¯
        signal_anomaly_data = {
            'wlan0_wireless_quality': 15.0,
            'wlan0_signal_level': -85.0,
            'wlan0_noise_level': -75.0,
            'wlan0_rx_packets': 8000,
            'wlan0_tx_packets': 5000,
            'wlan0_rx_bytes': 10000000,
            'wlan0_tx_bytes': 6000000,
            'gateway_ping_time': 150.0,
            'dns_resolution_time': 300.0,
            'memory_usage_percent': 40.0,
            'cpu_usage_percent': 25.0
        }
        
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        real_metrics = list(normal_data.keys())
        scaler_path = os.path.join(config['ai_models']['autoencoder']['model_path'], 'autoencoder_scaler.pkl')
        feature_extractor = FeatureExtractor(real_metrics, logger, scaler_path=scaler_path)
        
        # æå–ç‰¹å¾å¹¶åˆ†æ
        normal_features = feature_extractor.extract_features(normal_data)
        anomaly_features = feature_extractor.extract_features(signal_anomaly_data)
        
        print(f"åœºæ™¯ç‰¹å¾å¯¹æ¯”:")
        print(f"{'ç‰¹å¾':<12} {'è®­ç»ƒå‡å€¼':<12} {'æ­£å¸¸åœºæ™¯':<12} {'å¼‚å¸¸åœºæ™¯':<12} {'å¼‚å¸¸åç¦»':<12}")
        print("-" * 65)
        
        for i in range(len(normal_features)):
            training_mean = baseline[i]
            normal_val = normal_features[i]
            anomaly_val = anomaly_features[i]
            anomaly_deviation = abs(anomaly_val - training_mean) / abs(training_mean) if abs(training_mean) > 1e-6 else 0
            
            print(f"feature_{i:02d}  {training_mean:>11.2f} {normal_val:>11.2f} {anomaly_val:>11.2f} {anomaly_deviation:>11.2f}")
        
        return feature_stats, X
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    analyze_training_data() 