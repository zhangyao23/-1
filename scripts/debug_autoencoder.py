#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è°ƒè¯•è‡ªç¼–ç å™¨æ¨ç†è¿‡ç¨‹
åˆ†æä¸ºä»€ä¹ˆä¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒçš„é‡æ„è¯¯å·®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import joblib
import json
from src.ai_models.autoencoder_model import AutoencoderModel
from src.logger.system_logger import SystemLogger

def load_config():
    """åŠ è½½ç³»ç»Ÿé…ç½®"""
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'system_config.json')
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def debug_autoencoder_inference():
    """è°ƒè¯•è‡ªç¼–ç å™¨æ¨ç†è¿‡ç¨‹"""
    
    print("ğŸ” è°ƒè¯•è‡ªç¼–ç å™¨æ¨ç†è¿‡ç¨‹")
    print("="*50)
    
    try:
        # åŠ è½½é…ç½®
        config = load_config()
        logger = SystemLogger(config['logging'])
        logger.set_log_level('WARNING')  # å‡å°‘æ—¥å¿—è¾“å‡º
        
        # åˆå§‹åŒ–è‡ªç¼–ç å™¨
        autoencoder_config = config['ai_models']['autoencoder']
        autoencoder = AutoencoderModel(autoencoder_config, logger)
        
        # åŠ è½½scaler
        scaler_path = os.path.join(autoencoder_config['model_path'], 'autoencoder_scaler.pkl')
        scaler = joblib.load(scaler_path)
        
        print(f"âœ… æ¨¡å‹è·¯å¾„: {autoencoder_config['model_path']}")
        print(f"âœ… Scalerè·¯å¾„: {scaler_path}")
        print(f"âœ… å¼‚å¸¸é˜ˆå€¼: {autoencoder_config['threshold']}")
        
        # æµ‹è¯•ä¸åŒçš„è¾“å…¥åœºæ™¯
        test_scenarios = {
            "æ­£å¸¸åœºæ™¯": {
                "åŸå§‹ç‰¹å¾": [75.0, 1.5, 11.75, 0.005, 12.0, 35.0],
                "æœŸæœ›": "ä½é‡æ„è¯¯å·®"
            },
            "ä¿¡å·å¼‚å¸¸": {
                "åŸå§‹ç‰¹å¾": [25.0, 0.15, 150.0, 0.15, 85.0, 90.0],
                "æœŸæœ›": "é«˜é‡æ„è¯¯å·®"
            },
            "å…¨é›¶æµ‹è¯•": {
                "åŸå§‹ç‰¹å¾": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                "æœŸæœ›": "åŸºå‡†è¯¯å·®"
            },
            "å…¨ä¸€æµ‹è¯•": {
                "åŸå§‹ç‰¹å¾": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                "æœŸæœ›": "ä¸åŒè¯¯å·®"
            }
        }
        
        print("\nğŸ“Š æµ‹è¯•ä¸åŒè¾“å…¥åœºæ™¯:")
        print("-" * 50)
        
        for scenario_name, scenario_data in test_scenarios.items():
            print(f"\nğŸ§ª {scenario_name}:")
            
            # åŸå§‹6ç»´ç‰¹å¾
            features = np.array(scenario_data["åŸå§‹ç‰¹å¾"])
            print(f"   åŸå§‹ç‰¹å¾: {features}")
            
            # æ ‡å‡†åŒ–å‰çš„åˆ†æ
            print(f"   ç‰¹å¾èŒƒå›´: [{features.min():.2f}, {features.max():.2f}]")
            print(f"   ç‰¹å¾æ ‡å‡†å·®: {features.std():.4f}")
            
            # åº”ç”¨æ ‡å‡†åŒ–
            features_2d = features.reshape(1, -1)
            features_scaled = scaler.transform(features_2d)
            features_scaled_1d = features_scaled.flatten()
            
            print(f"   æ ‡å‡†åŒ–å: {features_scaled_1d}")
            print(f"   æ ‡å‡†åŒ–èŒƒå›´: [{features_scaled_1d.min():.4f}, {features_scaled_1d.max():.4f}]")
            print(f"   æ ‡å‡†åŒ–æ ‡å‡†å·®: {features_scaled_1d.std():.4f}")
            
            # è°ƒç”¨è‡ªç¼–ç å™¨
            result = autoencoder.predict(features_scaled)
            
            # åˆ†æç»“æœ
            recon_error = result.get('reconstruction_error', 0.0)
            is_anomaly = result.get('is_anomaly', False)
            
            print(f"   â¤ é‡æ„è¯¯å·®: {recon_error:.6f}")
            print(f"   â¤ æ˜¯å¦å¼‚å¸¸: {is_anomaly}")
            print(f"   â¤ æœŸæœ›ç»“æœ: {scenario_data['æœŸæœ›']}")
        
        print("\nğŸ” Scalerç»Ÿè®¡ä¿¡æ¯:")
        print("-" * 30)
        print(f"   Scalerç±»å‹: {type(scaler).__name__}")
        
        # RobustScalerçš„å±æ€§ä¸StandardScalerä¸åŒ
        if hasattr(scaler, 'center_'):
            print(f"   Center (ä¸­ä½æ•°): {scaler.center_}")
        if hasattr(scaler, 'scale_'):
            print(f"   Scale (IQR): {scaler.scale_}")
        if hasattr(scaler, 'mean_'):
            print(f"   Mean: {scaler.mean_}")
        
        # æ£€æŸ¥scaleræ˜¯å¦åˆç†
        if hasattr(scaler, 'scale_') and np.any(scaler.scale_ == 0):
            print("   âš ï¸  è­¦å‘Š: æŸäº›ç‰¹å¾çš„æ ‡å‡†å·®ä¸º0ï¼Œå¯èƒ½å¯¼è‡´æ•°å€¼é—®é¢˜ï¼")
            zero_scale_indices = np.where(scaler.scale_ == 0)[0]
            print(f"   é›¶æ ‡å‡†å·®ç‰¹å¾ç´¢å¼•: {zero_scale_indices}")
        
        # å…³é”®é—®é¢˜ï¼šåˆ†æä¸ºä»€ä¹ˆè‡ªç¼–ç å™¨æ€»æ˜¯è¾“å‡º0.000160
        print("\nğŸš¨ å…³é”®é—®é¢˜åˆ†æ:")
        print("-" * 30)
        print("   âŒ æ‰€æœ‰è¾“å…¥éƒ½äº§ç”Ÿç›¸åŒé‡æ„è¯¯å·®: 0.000160")
        print("   âŒ è¿™è¡¨æ˜è‡ªç¼–ç å™¨æ¨¡å‹æœ‰ä¸¥é‡ç¼ºé™·")
        print("   â¤ å¯èƒ½åŸå› :")
        print("      1. æ¨¡å‹è®­ç»ƒå¤±è´¥æˆ–è¿‡æ‹Ÿåˆ")
        print("      2. æ¨¡å‹æƒé‡å…¨ä¸ºé›¶æˆ–å¸¸æ•°")
        print("      3. æ¨¡å‹ä¿å­˜/åŠ è½½æœ‰é—®é¢˜")
        print("      4. SavedModelæ¨ç†å‡½æ•°å¼‚å¸¸")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*50)
    print("ğŸ¯ è°ƒè¯•å®Œæˆ")

if __name__ == "__main__":
    debug_autoencoder_inference() 