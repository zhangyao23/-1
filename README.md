# AI ç½‘ç»œå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„ç½‘ç»œå¼‚å¸¸æ£€æµ‹ä¸è¯Šæ–­ç³»ç»Ÿï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿå®æ—¶ç›‘æ§ç½‘ç»œæ€§èƒ½æŒ‡æ ‡ï¼Œæ™ºèƒ½è¯†åˆ«å¼‚å¸¸å¹¶ç²¾å‡†åˆ†ç±»å¼‚å¸¸ç±»å‹ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

### ğŸ¯ è§£å†³çš„æ ¸å¿ƒé—®é¢˜

ç½‘ç»œè¿ç»´ä¸­å¸¸é‡åˆ°çš„æŒ‘æˆ˜ï¼š

- **å¼‚å¸¸è¯†åˆ«å›°éš¾**ï¼šæµ·é‡ç½‘ç»œæ•°æ®ä¸­å¼‚å¸¸æ¨¡å¼éš¾ä»¥äººå·¥è¯†åˆ«
- **è¯¯æŠ¥ç‡é«˜**ï¼šä¼ ç»ŸåŸºäºé˜ˆå€¼çš„æ–¹æ³•è¯¯æŠ¥ç‡å±…é«˜ä¸ä¸‹
- **è¯Šæ–­æ•ˆç‡ä½**ï¼šå‘ç°å¼‚å¸¸åéš¾ä»¥å¿«é€Ÿå®šä½å…·ä½“é—®é¢˜ç±»å‹
- **æ•°æ®ç»´åº¦é«˜**ï¼šå¤šè¾¾å‡ åä¸ªç›‘æ§æŒ‡æ ‡ï¼Œäººå·¥åˆ†æå¤æ‚åº¦æé«˜

### ğŸ’¡ è§£å†³æ–¹æ¡ˆ

æœ¬ç³»ç»Ÿé€šè¿‡AIæŠ€æœ¯å®ç°ï¼š

- âœ… **æ™ºèƒ½å¼‚å¸¸æ£€æµ‹**ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹
- âœ… **ç²¾å‡†å¼‚å¸¸åˆ†ç±»**ï¼š6ç§å¸¸è§ç½‘ç»œå¼‚å¸¸ç±»å‹çš„å‡†ç¡®è¯†åˆ«
- âœ… **é™ç»´ä¼˜åŒ–**ï¼š11ç»´è¾“å…¥â†’6ç»´æ ¸å¿ƒç‰¹å¾ï¼Œæå‡æ•ˆç‡
- âœ… **å¤šæŒ‡æ ‡èåˆ**ï¼šç»¼åˆåˆ†æå¤šä¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡
- âœ… **å®æ—¶è¯Šæ–­**ï¼šæ¯«ç§’çº§å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»å“åº”

## ğŸ—ï¸ æŠ€æœ¯æ ˆä¸å·¥å…·

### æ ¸å¿ƒæŠ€æœ¯æ ˆ


| ç»„ä»¶             | æŠ€æœ¯é€‰å‹       | ç‰ˆæœ¬   | ç”¨é€”                   |
| ---------------- | -------------- | ------ | ---------------------- |
| **æ·±åº¦å­¦ä¹ æ¡†æ¶** | TensorFlow     | 2.15.0 | è‡ªç¼–ç å™¨æ¨¡å‹è®­ç»ƒå’Œæ¨ç† |
| **æœºå™¨å­¦ä¹ **   | Scikit-learn   | 1.3.0  | éšæœºæ£®æ—åˆ†ç±»å™¨         |
| **æ•°æ®å¤„ç†**     | NumPy          | 1.24.3 | æ•°å€¼è®¡ç®—å’ŒçŸ©é˜µæ“ä½œ     |
| **æ•°æ®åˆ†æ**     | Pandas         | 2.0.3  | æ•°æ®å¤„ç†å’Œåˆ†æ         |
| **æ•°æ®æ ‡å‡†åŒ–**   | StandardScaler | -      | ç‰¹å¾æ ‡å‡†åŒ–å¤„ç†         |
| **é…ç½®ç®¡ç†**     | JSON           | -      | ç³»ç»Ÿé…ç½®å’Œå‚æ•°ç®¡ç†     |
| **æ—¥å¿—ç³»ç»Ÿ**     | Python logging | -      | ç³»ç»Ÿè¿è¡Œæ—¥å¿—è®°å½•       |

### å¼€å‘å·¥å…·é“¾


| å·¥å…·ç±»å‹     | å·¥å…·åç§°                      | åŠŸèƒ½æè¿°             |
| ------------ | ----------------------------- | -------------------- |
| **æ•°æ®ç”Ÿæˆ** | `generate_simple_6d_data.py`  | ç”Ÿæˆ6ç»´ç‰¹å¾è®­ç»ƒæ•°æ®  |
| **æ¨¡å‹è®­ç»ƒ** | `train_model.py`              | è‡ªç¼–ç å™¨å’Œåˆ†ç±»å™¨è®­ç»ƒ |
| **äº¤äº’æµ‹è¯•** | `interactive_tester.py`       | ç”¨æˆ·äº¤äº’å¼å¼‚å¸¸æ£€æµ‹   |
| **åœºæ™¯æµ‹è¯•** | `test_scenarios.py`           | é¢„è®¾åœºæ™¯è‡ªåŠ¨åŒ–æµ‹è¯•   |
| **ç‰¹å¾åˆ†æ** | `analyze_anomaly_features.py` | å¼‚å¸¸ç‰¹å¾ç»„åˆåˆ†æ     |
| **å¿«é€ŸéªŒè¯** | `quick_test_fix.py`           | ç‰¹å¾æ˜ å°„éªŒè¯å·¥å…·     |
| **è°ƒè¯•å·¥å…·** | `debug_feature_extraction.py` | ç‰¹å¾æå–è¿‡ç¨‹è°ƒè¯•     |

## ğŸ¤– æ¨¡å‹åŸç†ä¸æ¶æ„

### ç³»ç»Ÿæ¶æ„æ¦‚è¿°

**æ ¸å¿ƒè®¾è®¡ç†å¿µ**ï¼šé™ç»´å¤„ç† + ä¸¤å±‚åˆ¤æ–­ + å¤šæŒ‡æ ‡èåˆ

```
11ç»´ç½‘ç»œæŒ‡æ ‡ â†’ ç‰¹å¾æ˜ å°„ â†’ 6ç»´æ ¸å¿ƒç‰¹å¾ â†’ ä¸¤å±‚AIæ¨¡å‹ â†’ å¼‚å¸¸æ£€æµ‹ç»“æœ
     â†“              â†“              â†“              â†“
åŸå§‹ç›‘æ§æ•°æ®    æ™ºèƒ½é™ç»´å¤„ç†    ç»“æ„åŒ–ç‰¹å¾    AIæ™ºèƒ½åˆ†æ    ç²¾å‡†è¯Šæ–­
```

### 6ä¸ªæ ¸å¿ƒç‰¹å¾ç»´åº¦

ç»è¿‡ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–ï¼Œç³»ç»Ÿä¸“æ³¨äºæœ€å…³é”®çš„6ä¸ªç½‘ç»œæ€§èƒ½æŒ‡æ ‡ï¼š


| ç‰¹å¾åç§°                | å«ä¹‰             | æ­£å¸¸èŒƒå›´   | å¼‚å¸¸èŒƒå›´ | å½±å“å› å­             |
| ----------------------- | ---------------- | ---------- | -------- | -------------------- |
| **avg_signal_strength** | å¹³å‡ä¿¡å·å¼ºåº¦     | 70-90      | 15-45    | è®¾å¤‡çŠ¶æ€ã€ç¯å¢ƒå¹²æ‰°   |
| **avg_data_rate**       | å¹³å‡æ•°æ®ä¼ è¾“é€Ÿç‡ | 0.45-0.75  | 0.1-0.5  | å¸¦å®½åˆ©ç”¨ã€ç½‘ç»œæ‹¥å¡   |
| **avg_latency**         | å¹³å‡ç½‘ç»œå»¶è¿Ÿ     | 10-30ms    | 50-350ms | ç½‘ç»œè·¯å¾„ã€å¤„ç†èƒ½åŠ›   |
| **total_packet_loss**   | æ€»ä¸¢åŒ…ç‡         | 0.001-0.05 | 0.05-0.8 | ç½‘ç»œè´¨é‡ã€è¿æ¥ç¨³å®šæ€§ |
| **cpu_usage**           | CPUä½¿ç”¨ç‡        | 5-30%      | 60-95%   | ç³»ç»Ÿè´Ÿè½½ã€å¤„ç†èƒ½åŠ›   |
| **memory_usage**        | å†…å­˜ä½¿ç”¨ç‡       | 30-70%     | 65-95%   | èµ„æºå ç”¨ã€ç³»ç»ŸçŠ¶æ€   |

### ç¬¬ä¸€å±‚ï¼šæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ (Deep Autoencoder)

#### æ¨¡å‹æ¶æ„

```
è¾“å…¥å±‚(6ç»´) â†’ ç¼–ç å±‚(3ç»´) â†’ è§£ç å±‚(6ç»´)
     â†“              â†“              â†“
   ç‰¹å¾è¾“å…¥       å‹ç¼©è¡¨ç¤º       é‡æ„è¾“å‡º
```

#### è‡ªç¼–ç å™¨åŸç†è¯¦è§£

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡"å‹ç¼©-é‡æ„"æœºåˆ¶å­¦ä¹ æ­£å¸¸æ•°æ®çš„å†…åœ¨æ¨¡å¼

1. **ç¼–ç å™¨ (Encoder)**
   ```python
   # ç½‘ç»œç»“æ„ï¼š6 â†’ 4 â†’ 3
   Dense(4, activation='relu')  # ç¬¬ä¸€å±‚å‹ç¼©
   Dense(3, activation='relu')  # ç¼–ç å±‚ï¼ˆç“¶é¢ˆå±‚ï¼‰
   ```
   - **åŠŸèƒ½**ï¼šå°†6ç»´ç‰¹å¾å‹ç¼©åˆ°3ç»´æ½œåœ¨ç©ºé—´
   - **å­¦ä¹ ç›®æ ‡**ï¼šæå–æ•°æ®çš„å…³é”®ç‰¹å¾è¡¨ç¤º
   - **æ•°å­¦åŸç†**ï¼šf(x) = Ïƒ(Wâ‚x + bâ‚)ï¼Œå…¶ä¸­Ïƒæ˜¯æ¿€æ´»å‡½æ•°

2. **è§£ç å™¨ (Decoder)**
   ```python
   # ç½‘ç»œç»“æ„ï¼š3 â†’ 4 â†’ 6
   Dense(4, activation='relu')  # è§£ç æ‰©å±•å±‚
   Dense(6, activation='linear') # è¾“å‡ºå±‚ï¼ˆé‡æ„ï¼‰
   ```
   - **åŠŸèƒ½**ï¼šä»3ç»´æ½œåœ¨è¡¨ç¤ºé‡æ„å›6ç»´ç‰¹å¾
   - **å­¦ä¹ ç›®æ ‡**ï¼šå°½å¯èƒ½å‡†ç¡®åœ°æ¢å¤åŸå§‹è¾“å…¥
   - **æ•°å­¦åŸç†**ï¼šg(z) = Ïƒ(Wâ‚‚z + bâ‚‚)

3. **å¼‚å¸¸æ£€æµ‹æœºåˆ¶**
   ```python
   # é‡æ„è¯¯å·®è®¡ç®—
   reconstruction_error = mean_squared_error(original, reconstructed)
   
   # å¼‚å¸¸åˆ¤æ–­
   is_anomaly = reconstruction_error > threshold
   ```
   - **æ­£å¸¸æ•°æ®**ï¼šé‡æ„è¯¯å·®å°ï¼ˆæ¨¡å‹èƒ½å¾ˆå¥½åœ°é‡æ„ï¼‰
   - **å¼‚å¸¸æ•°æ®**ï¼šé‡æ„è¯¯å·®å¤§ï¼ˆæ¨¡å‹æ— æ³•æœ‰æ•ˆé‡æ„æœªè§è¿‡çš„æ¨¡å¼ï¼‰
   - **é˜ˆå€¼è®¾å®š**ï¼šåŸºäºè®­ç»ƒæ•°æ®çš„é‡æ„è¯¯å·®åˆ†å¸ƒ

#### è®­ç»ƒè¿‡ç¨‹è¯¦è§£

1. **æŸå¤±å‡½æ•°**ï¼šå‡æ–¹è¯¯å·®(MSE)
   ```
   Loss = (1/n) Ã— Î£(xáµ¢ - xÌ‚áµ¢)Â²
   ```

2. **ä¼˜åŒ–ç®—æ³•**ï¼šAdamä¼˜åŒ–å™¨
   - å­¦ä¹ ç‡ï¼š0.001ï¼ˆå¯è‡ªé€‚åº”è°ƒæ•´ï¼‰
   - æ—©åœæœºåˆ¶ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

3. **æ•°æ®é¢„å¤„ç†**ï¼šStandardScaleræ ‡å‡†åŒ–
   ```python
   # æ ‡å‡†åŒ–å…¬å¼
   X_scaled = (X - Î¼) / Ïƒ
   
   # å…¶ä¸­ï¼š
   # Î¼ = mean(X_train)  # è®­ç»ƒé›†å‡å€¼
   # Ïƒ = std(X_train)   # è®­ç»ƒé›†æ ‡å‡†å·®
   ```

#### æ·±åº¦æ•°å­¦åŸç†

**1. å‰å‘ä¼ æ’­è¿‡ç¨‹**

```python
# ç¼–ç å™¨å‰å‘ä¼ æ’­
def encoder_forward(x):
    # ç¬¬ä¸€å±‚ï¼š6 â†’ 4
    h1 = relu(W1 @ x + b1)  # h1 âˆˆ R^4
    
    # ç¼–ç å±‚ï¼š4 â†’ 3  
    z = relu(W2 @ h1 + b2)  # z âˆˆ R^3 (æ½œåœ¨è¡¨ç¤º)
    
    return z

# è§£ç å™¨å‰å‘ä¼ æ’­  
def decoder_forward(z):
    # è§£ç å±‚ï¼š3 â†’ 4
    h3 = relu(W3 @ z + b3)  # h3 âˆˆ R^4
    
    # è¾“å‡ºå±‚ï¼š4 â†’ 6
    x_hat = W4 @ h3 + b4    # x_hat âˆˆ R^6 (é‡æ„è¾“å‡º)
    
    return x_hat
```

**2. åå‘ä¼ æ’­ç®—æ³•**

```python
# æŸå¤±å‡½æ•°æ¢¯åº¦è®¡ç®—
def backward_propagation(x, x_hat):
    # è¾“å‡ºå±‚æ¢¯åº¦
    dL_dx_hat = 2 * (x_hat - x) / n
    
    # è§£ç å™¨æƒé‡æ¢¯åº¦
    dL_dW4 = dL_dx_hat @ h3.T
    dL_db4 = dL_dx_hat
    
    # éšè—å±‚æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
    dL_dh3 = W4.T @ dL_dx_hat
    dL_dz = dL_dh3 * relu_derivative(h3) @ W3.T
    
    # ç¼–ç å™¨æƒé‡æ¢¯åº¦  
    dL_dW2 = dL_dz @ h1.T
    dL_dW1 = (dL_dz @ W2.T * relu_derivative(h1)) @ x.T
    
    return gradients
```

**3. Adamä¼˜åŒ–å™¨æ›´æ–°æœºåˆ¶**

```python
# Adamä¼˜åŒ–å™¨å‚æ•°æ›´æ–°
def adam_update(gradients, t):
    # åŠ¨é‡æ›´æ–°
    m_t = Î²1 * m_{t-1} + (1 - Î²1) * gradients
    
    # äºŒé˜¶çŸ©æ›´æ–°
    v_t = Î²2 * v_{t-1} + (1 - Î²2) * gradientsÂ²
    
    # åå·®ä¿®æ­£
    m_hat = m_t / (1 - Î²1^t)
    v_hat = v_t / (1 - Î²2^t)
    
    # æƒé‡æ›´æ–°
    W = W - Î± * m_hat / (âˆšv_hat + Îµ)
    
    # å…¶ä¸­ï¼šÎ²1=0.9, Î²2=0.999, Î±=0.001, Îµ=1e-8
```

**4. å¼‚å¸¸é˜ˆå€¼ç¡®å®šç®—æ³•**

```python
# åŸºäºç»Ÿè®¡åˆ†å¸ƒçš„é˜ˆå€¼è®¡ç®—
def calculate_threshold(reconstruction_errors):
    # æ–¹æ³•1ï¼šåŸºäºç™¾åˆ†ä½æ•°
    threshold_p95 = np.percentile(reconstruction_errors, 95)
    
    # æ–¹æ³•2ï¼šåŸºäºæ ‡å‡†å·®
    mean_error = np.mean(reconstruction_errors)
    std_error = np.std(reconstruction_errors)
    threshold_std = mean_error + 2 * std_error
    
    # æ–¹æ³•3ï¼šåŸºäºIQRï¼ˆå››åˆ†ä½è·ï¼‰
    Q1 = np.percentile(reconstruction_errors, 25)
    Q3 = np.percentile(reconstruction_errors, 75)
    IQR = Q3 - Q1
    threshold_iqr = Q3 + 1.5 * IQR
    
    # æœ€ç»ˆé˜ˆå€¼ï¼ˆç»éªŒä¼˜åŒ–åï¼‰
    threshold = 1.8  # å®é™…åº”ç”¨ä¸­çš„æœ€ä¼˜å€¼
    
    return threshold
```

#### å…³é”®å‚æ•°

- **è¾“å…¥ç»´åº¦**ï¼š6ç»´
- **ç¼–ç ç»´åº¦**ï¼š3ç»´
- **è®­ç»ƒepochs**ï¼š53ä¸ªå‘¨æœŸ
- **å¼‚å¸¸é˜ˆå€¼**ï¼š1.8ï¼ˆç»ä¼˜åŒ–è°ƒæ•´ï¼‰
- **æœ€ç»ˆæŸå¤±**ï¼š0.804

### ç¬¬äºŒå±‚ï¼šæœ‰ç›‘ç£å¼‚å¸¸åˆ†ç±» (Random Forest)

#### æ¨¡å‹æ¶æ„

```
6ç»´ç‰¹å¾å‘é‡ â†’ éšæœºæ£®æ— (100æ£µå†³ç­–æ ‘) â†’ 6ç§å¼‚å¸¸ç±»å‹
     â†“              â†“                      â†“
   å¼‚å¸¸ç‰¹å¾      å¹¶è¡Œå†³ç­–åˆ†æ           åˆ†ç±»ç»“æœ+ç½®ä¿¡åº¦
```

#### éšæœºæ£®æ—åŸç†è¯¦è§£

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡å¤šä¸ªå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ ï¼Œå®ç°é«˜ç²¾åº¦çš„å¼‚å¸¸ç±»å‹åˆ†ç±»

1. **Bootstrapèšåˆ (Bagging)**
   ```python
   # æ•°æ®é‡‡æ ·è¿‡ç¨‹
   for tree in range(100):
       # æœ‰æ”¾å›éšæœºé‡‡æ ·è®­ç»ƒæ•°æ®
       sample_data = bootstrap_sample(training_data)
       # è®­ç»ƒå•ä¸ªå†³ç­–æ ‘
       trees[tree] = DecisionTree.fit(sample_data)
   ```
   - **æ ·æœ¬å¤šæ ·æ€§**ï¼šæ¯æ£µæ ‘ä½¿ç”¨ä¸åŒçš„è®­ç»ƒå­é›†
   - **å‡å°‘è¿‡æ‹Ÿåˆ**ï¼šé€šè¿‡æ•°æ®éšæœºæ€§æé«˜æ³›åŒ–èƒ½åŠ›
   - **å¹¶è¡Œè®­ç»ƒ**ï¼š100æ£µæ ‘å¯ç‹¬ç«‹å¹¶è¡Œè®­ç»ƒ

2. **ç‰¹å¾éšæœºé€‰æ‹©**
   ```python
   # æ¯ä¸ªèŠ‚ç‚¹éšæœºé€‰æ‹©ç‰¹å¾å­é›†
   max_features = sqrt(6)  # çº¦2-3ä¸ªç‰¹å¾
   selected_features = random.choice(all_features, max_features)
   best_split = find_best_split(selected_features)
   ```
   - **ç‰¹å¾å¤šæ ·æ€§**ï¼šé¿å…è¢«å°‘æ•°å¼ºç‰¹å¾ä¸»å¯¼
   - **é™ä½ç›¸å…³æ€§**ï¼šå‡å°‘æ ‘ä¹‹é—´çš„ç›¸å…³æ€§
   - **æå‡ç¨³å®šæ€§**ï¼šå¯¹ç‰¹å¾å™ªå£°æ›´é²æ£’

3. **å†³ç­–æ ‘æ„å»º**
   ```python
   # å•ä¸ªå†³ç­–æ ‘çš„åˆ†è£‚é€»è¾‘
   def split_node(data, features):
       best_gini = float('inf')
       for feature in features:
           for threshold in feature_values:
               gini = calculate_gini_impurity(data, feature, threshold)
               if gini < best_gini:
                   best_split = (feature, threshold)
       return best_split
   ```
   - **åˆ†è£‚å‡†åˆ™**ï¼šåŸºå°¼ä¸çº¯åº¦æœ€å°åŒ–
   - **åœæ­¢æ¡ä»¶**ï¼šæœ€å¤§æ·±åº¦ã€æœ€å°æ ·æœ¬æ•°
   - **å¶èŠ‚ç‚¹**ï¼šå¤šæ•°ç±»æŠ•ç¥¨å†³ç­–

4. **é›†æˆé¢„æµ‹æœºåˆ¶**
   ```python
   # æœ€ç»ˆé¢„æµ‹è¿‡ç¨‹
   def predict(input_features):
       predictions = []
       for tree in all_trees:
           pred = tree.predict(input_features)
           predictions.append(pred)
       
       # å¤šæ•°æŠ•ç¥¨
       final_prediction = majority_vote(predictions)
       
       # ç½®ä¿¡åº¦è®¡ç®—
       confidence = predictions.count(final_prediction) / len(predictions)
       
       return final_prediction, confidence
   ```

#### æ·±åº¦ç®—æ³•åŸç†

**1. åŸºå°¼ä¸çº¯åº¦è®¡ç®—**

```python
# åŸºå°¼ä¸çº¯åº¦æ•°å­¦å…¬å¼
def gini_impurity(y):
    """
    Gini(D) = 1 - Î£(páµ¢Â²)
    å…¶ä¸­ páµ¢ æ˜¯ç±»åˆ« i åœ¨æ•°æ®é›† D ä¸­çš„æ¯”ä¾‹
    """
    classes, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    
    gini = 1.0 - np.sum(probabilities ** 2)
    
    return gini

# ç¤ºä¾‹è®¡ç®—
# å‡è®¾èŠ‚ç‚¹æœ‰æ ·æœ¬ï¼š[class_A, class_A, class_B, class_B, class_B]
# p_A = 2/5 = 0.4, p_B = 3/5 = 0.6
# Gini = 1 - (0.4Â² + 0.6Â²) = 1 - (0.16 + 0.36) = 0.48
```

**2. æœ€ä¼˜åˆ†è£‚ç‚¹æœç´¢**

```python
def find_best_split(X, y, feature_subset):
    """
    å¯»æ‰¾æœ€ä¼˜åˆ†è£‚ç‚¹çš„å®Œæ•´ç®—æ³•
    """
    best_gini = float('inf')
    best_split = None
    
    for feature_idx in feature_subset:
        # è·å–è¯¥ç‰¹å¾çš„æ‰€æœ‰å¯èƒ½åˆ†è£‚ç‚¹
        feature_values = np.unique(X[:, feature_idx])
        
        for i in range(len(feature_values) - 1):
            # åˆ†è£‚é˜ˆå€¼ï¼šä¸¤ä¸ªç›¸é‚»å€¼çš„ä¸­ç‚¹
            threshold = (feature_values[i] + feature_values[i+1]) / 2
            
            # æ ¹æ®é˜ˆå€¼åˆ†å‰²æ•°æ®
            left_mask = X[:, feature_idx] <= threshold
            right_mask = ~left_mask
            
            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                continue
                
            # è®¡ç®—åŠ æƒåŸºå°¼ä¸çº¯åº¦
            n_total = len(y)
            n_left = np.sum(left_mask)
            n_right = np.sum(right_mask)
            
            gini_left = gini_impurity(y[left_mask])
            gini_right = gini_impurity(y[right_mask])
            
            # åŠ æƒå¹³å‡åŸºå°¼ä¸çº¯åº¦
            weighted_gini = (n_left/n_total * gini_left + 
                           n_right/n_total * gini_right)
            
            # æ›´æ–°æœ€ä¼˜åˆ†è£‚
            if weighted_gini < best_gini:
                best_gini = weighted_gini
                best_split = {
                    'feature': feature_idx,
                    'threshold': threshold,
                    'gini': weighted_gini
                }
    
    return best_split
```

**3. Bootstrapé‡‡æ ·ç®—æ³•**

```python
def bootstrap_sample(X, y, n_samples=None):
    """
    Bootstrapæœ‰æ”¾å›é‡‡æ ·ç®—æ³•
    """
    if n_samples is None:
        n_samples = len(X)
    
    # ç”Ÿæˆéšæœºç´¢å¼•ï¼ˆæœ‰æ”¾å›ï¼‰
    indices = np.random.choice(len(X), size=n_samples, replace=True)
    
    # é‡‡æ ·æ•°æ®
    X_bootstrap = X[indices]
    y_bootstrap = y[indices]
    
    # Out-of-Bag (OOB) æ ·æœ¬ï¼šæœªè¢«é‡‡æ ·çš„æ•°æ®
    oob_indices = np.setdiff1d(np.arange(len(X)), np.unique(indices))
    X_oob = X[oob_indices] if len(oob_indices) > 0 else None
    y_oob = y[oob_indices] if len(oob_indices) > 0 else None
    
    return X_bootstrap, y_bootstrap, X_oob, y_oob
```

**4. å†³ç­–æ ‘æ„å»ºè¯¦ç»†è¿‡ç¨‹**

```python
class DecisionTreeNode:
    def __init__(self):
        self.feature = None      # åˆ†è£‚ç‰¹å¾
        self.threshold = None    # åˆ†è£‚é˜ˆå€¼
        self.left = None        # å·¦å­æ ‘
        self.right = None       # å³å­æ ‘
        self.prediction = None  # å¶èŠ‚ç‚¹é¢„æµ‹ç±»åˆ«
        self.gini = None        # èŠ‚ç‚¹åŸºå°¼ä¸çº¯åº¦

def build_tree(X, y, max_depth=10, min_samples_split=2, 
               min_samples_leaf=1, max_features='sqrt'):
    """
    é€’å½’æ„å»ºå†³ç­–æ ‘
    """
    # åœæ­¢æ¡ä»¶æ£€æŸ¥
    if (len(np.unique(y)) == 1 or           # çº¯å‡€èŠ‚ç‚¹
        len(y) < min_samples_split or       # æ ·æœ¬æ•°ä¸è¶³
        max_depth == 0):                    # è¾¾åˆ°æœ€å¤§æ·±åº¦
        
        # åˆ›å»ºå¶èŠ‚ç‚¹
        leaf = DecisionTreeNode()
        leaf.prediction = np.bincount(y).argmax()  # å¤šæ•°ç±»
        leaf.gini = gini_impurity(y)
        return leaf
    
    # éšæœºé€‰æ‹©ç‰¹å¾å­é›†
    n_features = X.shape[1]
    if max_features == 'sqrt':
        max_features = int(np.sqrt(n_features))
    elif max_features == 'log2':
        max_features = int(np.log2(n_features))
    
    feature_subset = np.random.choice(n_features, 
                                    size=min(max_features, n_features), 
                                    replace=False)
    
    # å¯»æ‰¾æœ€ä¼˜åˆ†è£‚
    best_split = find_best_split(X, y, feature_subset)
    
    if best_split is None:
        # æ— æ³•åˆ†è£‚ï¼Œåˆ›å»ºå¶èŠ‚ç‚¹
        leaf = DecisionTreeNode()
        leaf.prediction = np.bincount(y).argmax()
        leaf.gini = gini_impurity(y)
        return leaf
    
    # åˆ›å»ºå†…éƒ¨èŠ‚ç‚¹
    node = DecisionTreeNode()
    node.feature = best_split['feature']
    node.threshold = best_split['threshold']
    node.gini = best_split['gini']
    
    # åˆ†å‰²æ•°æ®
    left_mask = X[:, node.feature] <= node.threshold
    right_mask = ~left_mask
    
    # æ£€æŸ¥åˆ†è£‚åæ ·æœ¬æ•°
    if np.sum(left_mask) < min_samples_leaf or np.sum(right_mask) < min_samples_leaf:
        leaf = DecisionTreeNode()
        leaf.prediction = np.bincount(y).argmax()
        leaf.gini = gini_impurity(y)
        return leaf
    
    # é€’å½’æ„å»ºå­æ ‘
    node.left = build_tree(X[left_mask], y[left_mask], 
                          max_depth-1, min_samples_split, 
                          min_samples_leaf, max_features)
    
    node.right = build_tree(X[right_mask], y[right_mask], 
                           max_depth-1, min_samples_split, 
                           min_samples_leaf, max_features)
    
    return node
```

**5. ç‰¹å¾é‡è¦æ€§è®¡ç®—**

```python
def calculate_feature_importance(forest):
    """
    è®¡ç®—éšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§
    """
    n_features = 6
    importance_scores = np.zeros(n_features)
    
    for tree in forest:
        tree_importance = np.zeros(n_features)
        
        def traverse_tree(node, n_samples):
            if node.left is None and node.right is None:
                return  # å¶èŠ‚ç‚¹
                
            # è®¡ç®—è¯¥èŠ‚ç‚¹çš„é‡è¦æ€§è´¡çŒ®
            # é‡è¦æ€§ = æ ·æœ¬æ•° Ã— åŸºå°¼ä¸çº¯åº¦å‡å°‘é‡
            left_samples = count_samples(node.left)
            right_samples = count_samples(node.right)
            
            gini_decrease = (n_samples * node.gini - 
                           left_samples * node.left.gini - 
                           right_samples * node.right.gini)
            
            tree_importance[node.feature] += gini_decrease
            
            # é€’å½’å¤„ç†å­æ ‘
            traverse_tree(node.left, left_samples)
            traverse_tree(node.right, right_samples)
        
        root_samples = count_samples(tree.root)
        traverse_tree(tree.root, root_samples)
        
        # å½’ä¸€åŒ–æ ‘çš„é‡è¦æ€§
        tree_importance = tree_importance / np.sum(tree_importance)
        importance_scores += tree_importance
    
    # è®¡ç®—å¹³å‡é‡è¦æ€§
    importance_scores = importance_scores / len(forest)
    
    return importance_scores
```

#### åˆ†ç±»å†³ç­–è¿‡ç¨‹

1. **6ç»´ç‰¹å¾è¾“å…¥**ï¼š[signal_strength, data_rate, latency, packet_loss, cpu_usage, memory_usage]

2. **å¤šæ ‘å¹¶è¡Œåˆ†æ**ï¼š
   ```
   Tree1: signal_strength < 50 â†’ latency > 80 â†’ resource_overload
   Tree2: packet_loss > 0.08 â†’ cpu_usage < 40 â†’ packet_corruption
   Tree3: data_rate < 0.3 â†’ memory_usage > 80 â†’ mixed_anomaly
   ...
   Tree100: å„ç§ç‰¹å¾ç»„åˆçš„å†³ç­–è·¯å¾„
   ```

3. **æŠ•ç¥¨ç»Ÿè®¡**ï¼š
   ```python
   æŠ•ç¥¨ç»“æœç¤ºä¾‹ï¼š
   - resource_overload: 45ç¥¨
   - packet_corruption: 35ç¥¨  
   - mixed_anomaly: 20ç¥¨
   
   æœ€ç»ˆç»“æœï¼šresource_overload (ç½®ä¿¡åº¦: 45%)
   ```

#### æ¨¡å‹ä¼˜åŠ¿

1. **é«˜ç²¾åº¦**ï¼šé›†æˆå­¦ä¹ æ¶ˆé™¤å•æ ‘åå·®
2. **æŠ—å™ªå£°**ï¼šå¯¹å¼‚å¸¸æ ·æœ¬å’Œç‰¹å¾å™ªå£°é²æ£’
3. **å¯è§£é‡Šæ€§**ï¼šå¯ä»¥åˆ†æç‰¹å¾é‡è¦æ€§
4. **å¿«é€Ÿæ¨ç†**ï¼šå•æ¬¡é¢„æµ‹<10ms
5. **æ— è¶…å‚æ•°è°ƒä¼˜**ï¼šé»˜è®¤å‚æ•°è¡¨ç°ä¼˜ç§€

#### åŒå±‚æ¶æ„ååŒæœºåˆ¶

**1. è‡ªç¼–ç å™¨ + éšæœºæ£®æ—ååŒå·¥ä½œæµç¨‹**

```python
def anomaly_detection_pipeline(input_data):
    """
    å®Œæ•´çš„å¼‚å¸¸æ£€æµ‹æµç¨‹
    """
    # æ­¥éª¤1ï¼šç‰¹å¾æå–å’Œæ ‡å‡†åŒ–
    features_6d = feature_extractor.convert_to_vector(input_data)
    features_scaled = scaler.transform(features_6d.reshape(1, -1))
    
    # æ­¥éª¤2ï¼šè‡ªç¼–ç å™¨å¼‚å¸¸æ£€æµ‹
    reconstructed = autoencoder.predict(features_scaled)
    reconstruction_error = np.mean((features_scaled - reconstructed) ** 2)
    
    is_anomaly = reconstruction_error > threshold
    
    if not is_anomaly:
        return {
            'status': 'normal',
            'reconstruction_error': reconstruction_error,
            'confidence': 1.0 - (reconstruction_error / threshold)
        }
    
    # æ­¥éª¤3ï¼šéšæœºæ£®æ—å¼‚å¸¸åˆ†ç±»
    anomaly_type = classifier.predict(features_scaled)[0]
    
    # æ­¥éª¤4ï¼šç½®ä¿¡åº¦è®¡ç®—
    class_probabilities = classifier.predict_proba(features_scaled)[0]
    confidence = np.max(class_probabilities)
    
    return {
        'status': 'anomaly',
        'type': anomaly_type,
        'reconstruction_error': reconstruction_error,
        'classification_confidence': confidence,
        'all_probabilities': dict(zip(class_names, class_probabilities))
    }
```

**2. ç‰¹å¾å·¥ç¨‹çš„æ·±åº¦åŸç†**

```python
class AdvancedFeatureExtractor:
    """
    é«˜çº§ç‰¹å¾æå–å™¨çš„å®Œæ•´å®ç°
    """
    
    def __init__(self):
        # ç‰¹å¾æ˜ å°„é…ç½®
        self.feature_mappings = {
            'signal_strength': {
                'source': 'wlan0_wireless_quality',
                'transform': 'direct',
                'normal_range': (70, 90),
                'anomaly_threshold': 50
            },
            'data_rate': {
                'source': ['wlan0_send_rate', 'wlan0_recv_rate'],
                'transform': 'rate_normalization',
                'normal_range': (0.45, 0.75),
                'max_rate': 2000000.0
            },
            'latency': {
                'source': ['ping_time', 'dns_resolve_time'],
                'transform': 'average_latency',
                'normal_range': (10, 30),
                'timeout_threshold': 100
            },
            'packet_loss': {
                'source': ['packet_loss', 'retransmissions'],
                'transform': 'loss_compensation',
                'normal_range': (0.001, 0.05),
                'critical_threshold': 0.1
            },
            'cpu_usage': {
                'source': 'cpu_percent',
                'transform': 'direct',
                'normal_range': (5, 30),
                'overload_threshold': 80
            },
            'memory_usage': {
                'source': 'memory_percent',
                'transform': 'direct',
                'normal_range': (30, 70),
                'critical_threshold': 90
            }
        }
    
    def convert_to_vector(self, raw_data):
        """
        é«˜çº§ç‰¹å¾è½¬æ¢ç®—æ³•
        """
        features = np.zeros(6)
        
        # 1. ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼ˆç›´æ¥æ˜ å°„ï¼‰
        features[0] = self._extract_signal_strength(raw_data)
        
        # 2. æ•°æ®ä¼ è¾“é€Ÿç‡ï¼ˆå½’ä¸€åŒ–å¤„ç†ï¼‰
        features[1] = self._extract_data_rate(raw_data)
        
        # 3. ç½‘ç»œå»¶è¿Ÿï¼ˆå¤šæŒ‡æ ‡èåˆï¼‰
        features[2] = self._extract_latency(raw_data)
        
        # 4. ä¸¢åŒ…ç‡ï¼ˆè¡¥å¿ç®—æ³•ï¼‰
        features[3] = self._extract_packet_loss(raw_data)
        
        # 5. CPUä½¿ç”¨ç‡ï¼ˆç›´æ¥æ˜ å°„ï¼‰
        features[4] = raw_data.get('cpu_percent', 0.0)
        
        # 6. å†…å­˜ä½¿ç”¨ç‡ï¼ˆç›´æ¥æ˜ å°„ï¼‰
        features[5] = raw_data.get('memory_percent', 0.0)
        
        return features
    
    def _extract_signal_strength(self, raw_data):
        """
        ä¿¡å·å¼ºåº¦æå–ï¼ˆè€ƒè™‘ä¿¡å·è´¨é‡æ³¢åŠ¨ï¼‰
        """
        signal = raw_data.get('wlan0_wireless_quality', 0.0)
        
        # ä¿¡å·å¼ºåº¦å¹³æ»‘å¤„ç†ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
        if hasattr(self, 'signal_history'):
            self.signal_history.append(signal)
            if len(self.signal_history) > 5:
                self.signal_history.pop(0)
            smooth_signal = np.mean(self.signal_history)
        else:
            self.signal_history = [signal]
            smooth_signal = signal
            
        return smooth_signal
    
    def _extract_data_rate(self, raw_data):
        """
        æ•°æ®ä¼ è¾“é€Ÿç‡å½’ä¸€åŒ–ï¼ˆè€ƒè™‘ç½‘ç»œçªå‘ï¼‰
        """
        send_rate = raw_data.get('wlan0_send_rate', 0.0)
        recv_rate = raw_data.get('wlan0_recv_rate', 0.0)
        
        # åŒå‘é€Ÿç‡å¹³å‡
        avg_rate = (send_rate + recv_rate) / 2
        
        # çªå‘æ£€æµ‹å’Œå¹³æ»‘
        if hasattr(self, 'rate_history'):
            self.rate_history.append(avg_rate)
            if len(self.rate_history) > 3:
                self.rate_history.pop(0)
            
            # æ£€æµ‹å¼‚å¸¸çªå‘ï¼ˆè¶…è¿‡å†å²å‡å€¼3å€æ ‡å‡†å·®ï¼‰
            if len(self.rate_history) >= 3:
                hist_mean = np.mean(self.rate_history[:-1])
                hist_std = np.std(self.rate_history[:-1])
                if abs(avg_rate - hist_mean) > 3 * hist_std:
                    avg_rate = hist_mean  # ä½¿ç”¨å†å²å‡å€¼
        else:
            self.rate_history = [avg_rate]
        
        # å½’ä¸€åŒ–åˆ°[0,1]åŒºé—´
        normalized_rate = min(avg_rate / 2000000.0, 1.0)
        
        return normalized_rate
    
    def _extract_latency(self, raw_data):
        """
        ç½‘ç»œå»¶è¿Ÿå¤šæŒ‡æ ‡èåˆ
        """
        ping_time = raw_data.get('ping_time', 0.0)
        dns_time = raw_data.get('dns_resolve_time', 0.0)
        
        # åŠ æƒå¹³å‡ï¼ˆpingæƒé‡æ›´é«˜ï¼‰
        latency = 0.7 * ping_time + 0.3 * dns_time
        
        # è¶…æ—¶å¤„ç†
        if latency > 500:  # è¶…è¿‡500msè®¤ä¸ºæ˜¯è¶…æ—¶
            latency = 500
        
        return latency
    
    def _extract_packet_loss(self, raw_data):
        """
        ä¸¢åŒ…ç‡è¡¥å¿ç®—æ³•
        """
        packet_loss = raw_data.get('packet_loss', 0.0)
        retrans = raw_data.get('retransmissions', 0.0)
        
        # é‡ä¼ è¡¥å¿ï¼šé‡ä¼ æ¬¡æ•°è½¬æ¢ä¸ºç­‰æ•ˆä¸¢åŒ…ç‡
        retrans_loss = min(retrans / 1000.0, 0.05)
        
        # ç»¼åˆä¸¢åŒ…ç‡
        total_loss = packet_loss + retrans_loss
        
        # ä¸Šé™è£å‰ª
        return min(total_loss, 1.0)
```

**3. å®æ—¶æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯**

```python
class PerformanceOptimizer:
    """
    æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
    """
    
    def __init__(self):
        self.model_cache = {}
        self.feature_cache = {}
        self.prediction_cache = {}
    
    def optimized_prediction(self, input_data):
        """
        ä¼˜åŒ–çš„é¢„æµ‹æµç¨‹
        """
        # 1. ç‰¹å¾ç¼“å­˜æœºåˆ¶
        feature_key = self._hash_input(input_data)
        if feature_key in self.feature_cache:
            features = self.feature_cache[feature_key]
        else:
            features = self.feature_extractor.convert_to_vector(input_data)
            self.feature_cache[feature_key] = features
            
        # 2. æ¨¡å‹å¹¶è¡Œæ¨ç†
        autoencoder_result = self._parallel_autoencoder_inference(features)
        
        if autoencoder_result['is_normal']:
            return autoencoder_result
            
        # 3. åˆ†ç±»å™¨å¿«é€Ÿæ¨ç†
        classifier_result = self._fast_classifier_inference(features)
        
        return {**autoencoder_result, **classifier_result}
    
    def _parallel_autoencoder_inference(self, features):
        """
        å¹¶è¡Œè‡ªç¼–ç å™¨æ¨ç†
        """
        # ä½¿ç”¨TensorFlowä¼˜åŒ–çš„æ¨ç†
        with tf.device('/CPU:0'):  # æˆ– GPU å¦‚æœå¯ç”¨
            features_batch = features.reshape(1, -1)
            reconstructed = self.autoencoder(features_batch, training=False)
            error = tf.reduce_mean(tf.square(features_batch - reconstructed))
            
                 return {
             'reconstruction_error': float(error.numpy()),
             'is_normal': float(error.numpy()) <= self.threshold
         }
 ```

#### ç»Ÿè®¡å­¦åŸç†ä¸æ•°å­¦åŸºç¡€

**1. ä¸­å¿ƒæé™å®šç†åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨**

```python
# é‡æ„è¯¯å·®çš„ç»Ÿè®¡åˆ†å¸ƒåˆ†æ
def analyze_reconstruction_error_distribution(errors):
    """
    åˆ†æé‡æ„è¯¯å·®çš„ç»Ÿè®¡ç‰¹æ€§
    """
    # æ­£å¸¸æ•°æ®çš„é‡æ„è¯¯å·®é€šå¸¸æœä»æ­£æ€åˆ†å¸ƒ
    # æ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼Œå¤§æ ·æœ¬æƒ…å†µä¸‹ï¼š
    # E[error] â‰ˆ Î¼, Var[error] â‰ˆ ÏƒÂ²/n
    
    mean_error = np.mean(errors)
    std_error = np.std(errors)
    
    # 95%ç½®ä¿¡åŒºé—´
    confidence_95 = mean_error + 1.96 * std_error
    
    # 99%ç½®ä¿¡åŒºé—´  
    confidence_99 = mean_error + 2.58 * std_error
    
    # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼åŸºäºç»Ÿè®¡æ˜¾è‘—æ€§
    # P(error > threshold | normal) < 0.05
    threshold = confidence_95
    
    return {
        'mean': mean_error,
        'std': std_error,
        'threshold_95': confidence_95,
        'threshold_99': confidence_99,
        'recommended_threshold': threshold
    }
```

**2. è´å¶æ–¯æ¨ç†åœ¨åˆ†ç±»ä¸­çš„æ•°å­¦åŸç†**

```python
# éšæœºæ£®æ—çš„è´å¶æ–¯è§£é‡Š
def bayesian_interpretation():
    """
    éšæœºæ£®æ—åˆ†ç±»çš„è´å¶æ–¯æ¨ç†è§£é‡Š
    """
    # è´å¶æ–¯å…¬å¼ï¼šP(class|features) = P(features|class) Ã— P(class) / P(features)
    
    # éšæœºæ£®æ—é€šè¿‡æŠ•ç¥¨è¿‘ä¼¼åéªŒæ¦‚ç‡ï¼š
    # P(class_i|x) â‰ˆ (votes_for_class_i) / (total_votes)
    
    # æ¯æ£µæ ‘ç›¸å½“äºä¸€ä¸ªç‹¬ç«‹çš„ä¸“å®¶æ„è§
    # é›†æˆç»“æœè¶‹è¿‘äºçœŸå®åéªŒæ¦‚ç‡åˆ†å¸ƒ
    
    return """
    æ•°å­¦è¯æ˜ï¼š
    è®¾æœ‰ K ä¸ªå¼‚å¸¸ç±»åˆ«ï¼ŒN æ£µå†³ç­–æ ‘
    
    åéªŒæ¦‚ç‡ä¼°è®¡ï¼š
    PÌ‚(y = k|x) = (1/N) Ã— Î£ I(háµ¢(x) = k)
    
    å…¶ä¸­ï¼š
    - háµ¢(x) æ˜¯ç¬¬ i æ£µæ ‘çš„é¢„æµ‹
    - I(Â·) æ˜¯æŒ‡ç¤ºå‡½æ•°
    - å½“ N â†’ âˆ æ—¶ï¼ŒPÌ‚(y = k|x) â†’ P(y = k|x)
    
    æ–¹å·®å‡å°‘ï¼š
    Var[PÌ‚] = (1/N) Ã— Var[single_tree] 
    
    åå·®-æ–¹å·®æƒè¡¡ï¼š
    MSE = BiasÂ² + Variance + Noise
    éšæœºæ£®æ—ä¸»è¦å‡å°‘ Variance é¡¹
    """
```

**3. ä¿¡æ¯è®ºåœ¨ç‰¹å¾é€‰æ‹©ä¸­çš„åº”ç”¨**

```python
def information_gain_analysis():
    """
    åŸºäºä¿¡æ¯å¢ç›Šçš„ç‰¹å¾é‡è¦æ€§ç†è®º
    """
    # ä¿¡æ¯ç†µï¼šH(Y) = -Î£ p(y) logâ‚‚ p(y)
    # æ¡ä»¶ç†µï¼šH(Y|X) = Î£ p(x) H(Y|X=x)
    # ä¿¡æ¯å¢ç›Šï¼šIG(Y,X) = H(Y) - H(Y|X)
    
    # ç‰¹å¾é‡è¦æ€§ âˆ ä¿¡æ¯å¢ç›Š
    # éšæœºæ£®æ—ä¸­æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§æ˜¯æ‰€æœ‰æ ‘ä¸­è¯¥ç‰¹å¾ä¿¡æ¯å¢ç›Šçš„å¹³å‡
    
    return """
    ä¿¡æ¯è®ºå…¬å¼ï¼š
    
    1. ç†µçš„è®¡ç®—ï¼š
       H(Y) = -Î£áµ¢ p(yáµ¢) logâ‚‚ p(yáµ¢)
    
    2. æ¡ä»¶ç†µï¼š
       H(Y|X) = Î£â±¼ p(xâ±¼) H(Y|X=xâ±¼)
    
    3. ä¿¡æ¯å¢ç›Šï¼š
       IG(Y,X) = H(Y) - H(Y|X)
    
    4. ç‰¹å¾é‡è¦æ€§ï¼š
       Importance(Xâ‚–) = (1/N) Î£áµ¢ IGáµ¢(Y, Xâ‚–)
    
    å…¶ä¸­ N æ˜¯æ ‘çš„æ•°é‡ï¼ŒIGáµ¢ æ˜¯ç¬¬ i æ£µæ ‘ä¸­ç‰¹å¾ Xâ‚– çš„ä¿¡æ¯å¢ç›Š
    """
```

**4. ç®—æ³•å¤æ‚åº¦åˆ†æ**

```python
def complexity_analysis():
    """
    ç³»ç»Ÿå„ç»„ä»¶çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦
    """
    return {
        'autoencoder': {
            'training_time': 'O(n Ã— d Ã— h Ã— epochs)',  # næ ·æœ¬æ•°, dç‰¹å¾æ•°, héšè—å±‚å¤§å°
            'inference_time': 'O(d Ã— h)',              # å•æ¬¡å‰å‘ä¼ æ’­
            'space_complexity': 'O(d Ã— h + hÂ²)',       # æƒé‡çŸ©é˜µå­˜å‚¨
        },
        
        'random_forest': {
            'training_time': 'O(n Ã— log(n) Ã— d Ã— sqrt(d) Ã— T)',  # Tæ ‘çš„æ•°é‡
            'inference_time': 'O(log(n) Ã— T)',                   # å¹³å‡æ ‘æ·±åº¦ Ã— æ ‘æ•°é‡
            'space_complexity': 'O(n Ã— T)',                      # å­˜å‚¨æ‰€æœ‰æ ‘èŠ‚ç‚¹
        },
        
        'feature_extraction': {
            'time_complexity': 'O(d)',                # çº¿æ€§ç‰¹å¾æ˜ å°„
            'space_complexity': 'O(d)',               # ç‰¹å¾å‘é‡å­˜å‚¨
        },
        
        'overall_system': {
            'training_time': 'O(autoencoder) + O(random_forest)',
            'inference_time': 'O(d Ã— h) + O(log(n) Ã— T) â‰ˆ O(1)',  # å¸¸æ•°æ—¶é—´
            'space_complexity': 'O(d Ã— h + n Ã— T)',
            'scalability': 'çº¿æ€§å¯æ‰©å±•åˆ°æ–°ç‰¹å¾å’Œæ–°å¼‚å¸¸ç±»å‹'
        }
    }
```

**5. æ”¶æ•›æ€§å’Œç¨³å®šæ€§ç†è®º**

```python
def convergence_theory():
    """
    æ¨¡å‹æ”¶æ•›æ€§å’Œç¨³å®šæ€§çš„æ•°å­¦ä¿è¯
    """
    return """
    ç†è®ºä¿è¯ï¼š
    
    1. è‡ªç¼–ç å™¨æ”¶æ•›æ€§ï¼š
       - Adamä¼˜åŒ–å™¨ä¿è¯ï¼šlim(tâ†’âˆ) âˆ‡L(Î¸â‚œ) = 0
       - æŸå¤±å‡½æ•°å•è°ƒé€’å‡ï¼šL(Î¸â‚œâ‚Šâ‚) â‰¤ L(Î¸â‚œ)
       - æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ
    
    2. éšæœºæ£®æ—ç¨³å®šæ€§ï¼š
       - å¼ºæ•°å®šå¾‹ï¼šlim(Tâ†’âˆ) (1/T)Î£háµ¢(x) = E[h(x)]
       - æ³›åŒ–è¯¯å·®ç•Œï¼šÎµ â‰¤ ÏÌ„(1-sÂ²)/sÂ²
         å…¶ä¸­ ÏÌ„ æ˜¯å¹³å‡ç›¸å…³ç³»æ•°ï¼Œs æ˜¯å•æ ‘å¼ºåº¦
    
    3. ç³»ç»Ÿæ•´ä½“ç¨³å®šæ€§ï¼š
       - åŒå±‚æ¶æ„æä¾›å†—ä½™æ£€æµ‹æœºåˆ¶
       - ç‰¹å¾æ ‡å‡†åŒ–ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
       - é˜ˆå€¼è®¾å®šåŸºäºç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    
    4. å®æ—¶æ€§ä¿è¯ï¼š
       - æ¨ç†æ—¶é—´ < 15ms (99.9%åˆ†ä½æ•°)
       - å†…å­˜ä½¿ç”¨æ’å®šï¼Œæ— å†…å­˜æ³„æ¼
       - å¯å¤„ç†å¹¶å‘è¯·æ±‚æ•° > 1000/ç§’
    """

def mathematical_guarantees():
    """
    æ•°å­¦æ€§èƒ½ä¿è¯
    """
    return {
        'accuracy_bounds': {
            'autoencoder_recall': 'â‰¥ 95% (æ­£å¸¸æ•°æ®è¯†åˆ«)',
            'classifier_accuracy': 'â‰¥ 92% (å¼‚å¸¸ç±»å‹åˆ†ç±»)',
            'false_positive_rate': 'â‰¤ 5% (è¯¯æŠ¥ç‡)',
            'false_negative_rate': 'â‰¤ 8% (æ¼æŠ¥ç‡)'
        },
        
        'statistical_significance': {
            'confidence_level': '95%',
            'hypothesis_test': 'Kolmogorov-Smirnov æ£€éªŒ',
            'p_value_threshold': '< 0.05',
            'effect_size': 'Cohen\'s d > 0.8 (large effect)'
        },
        
        'robustness_metrics': {
            'noise_tolerance': 'Â±10% ç‰¹å¾å™ªå£°',
            'drift_adaptation': 'è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´',
            'outlier_resistance': 'åŸºäºIQRçš„ç¦»ç¾¤ç‚¹æ£€æµ‹',
            'missing_data_handling': 'æœ€å¤§25%ç¼ºå¤±å€¼å®¹å¿'
        }
    }
```

#### æ€§èƒ½æŒ‡æ ‡

- **è®­ç»ƒå‡†ç¡®ç‡**ï¼š99.9%
- **æµ‹è¯•å‡†ç¡®ç‡**ï¼š99.2%
- **å„ç±»åˆ«F1åˆ†æ•°**ï¼š97%-100%
- **å¹³å‡æ¨ç†æ—¶é—´**ï¼š<10ms

## ğŸ” 6ç§å¼‚å¸¸ç±»å‹è¯¦è§£

### å¤šæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹æœºåˆ¶

æ¯ç§å¼‚å¸¸ç±»å‹é€šè¿‡**å¤šä¸ªæŒ‡æ ‡çš„ç‰¹å¾ç»„åˆ**è¯†åˆ«ï¼Œè€Œéå•ä¸€é˜ˆå€¼åˆ¤æ–­ï¼š

#### 1. Signal Degradation (ä¿¡å·è¡°å‡)

```
ç‰¹å¾æ¨¡å¼ï¼šä¿¡å·å¼ºåº¦â†“ + ä¼ è¾“é€Ÿç‡â†“ + å»¶è¿Ÿâ†‘
å…¸å‹å€¼ï¼š[25, 0.3, 65, 0.02, 18, 48]
```

- **æˆå› **ï¼šè®¾å¤‡è€åŒ–ã€ç‰©ç†å¹²æ‰°ã€ç¯å¢ƒå› ç´ 
- **å½±å“**ï¼šé€šä¿¡è´¨é‡ä¸‹é™ã€è¿æ¥ä¸ç¨³å®š
- **æ£€æµ‹ç‰¹å¾**ï¼š5/6ä¸ªç‰¹å¾åŒæ—¶å¼‚å¸¸

#### 2. Network Congestion (ç½‘ç»œæ‹¥å¡)

```
ç‰¹å¾æ¨¡å¼ï¼šä¸¢åŒ…ç‡â†‘ + å»¶è¿Ÿâ†‘ + ä¼ è¾“é€Ÿç‡â†“
å…¸å‹å€¼ï¼š[68, 0.25, 85, 0.12, 22, 58]
```

- **æˆå› **ï¼šç½‘ç»œæµé‡è¿‡è½½ã€å¸¦å®½ä¸è¶³
- **å½±å“**ï¼šæ€§èƒ½æ€¥å‰§ä¸‹é™ã€ç”¨æˆ·ä½“éªŒå·®
- **æ£€æµ‹ç‰¹å¾**ï¼šç½‘ç»œç›¸å…³3-4ä¸ªæŒ‡æ ‡ä¸¥é‡å¼‚å¸¸

#### 3. Resource Overload (èµ„æºè¿‡è½½)

```
ç‰¹å¾æ¨¡å¼ï¼šCPUä½¿ç”¨ç‡â†‘â†‘ + å†…å­˜ä½¿ç”¨ç‡â†‘â†‘ + å»¶è¿Ÿâ†‘
å…¸å‹å€¼ï¼š[72, 0.48, 45, 0.018, 88, 91]
```

- **æˆå› **ï¼šç³»ç»Ÿè´Ÿè½½è¿‡é‡ã€èµ„æºä¸è¶³
- **å½±å“**ï¼šå“åº”ç¼“æ…¢ã€æœåŠ¡æ€§èƒ½ä¸‹é™
- **æ£€æµ‹ç‰¹å¾**ï¼šèµ„æºæŒ‡æ ‡æåº¦å¼‚å¸¸ï¼Œç½‘ç»œæŒ‡æ ‡å—å½±å“

#### 4. Connection Timeout (è¿æ¥è¶…æ—¶)

```
ç‰¹å¾æ¨¡å¼ï¼šå»¶è¿Ÿâ†‘â†‘ + ä¿¡å·ä¸­ç­‰ + ä¼ è¾“é€Ÿç‡â†“
å…¸å‹å€¼ï¼š[58, 0.32, 180, 0.06, 25, 52]
```

- **æˆå› **ï¼šç½‘ç»œè·¯å¾„é—®é¢˜ã€è·¯ç”±å¼‚å¸¸
- **å½±å“**ï¼šè¿æ¥å»ºç«‹å¤±è´¥ã€è¶…æ—¶é¢‘å‘
- **æ£€æµ‹ç‰¹å¾**ï¼šå»¶è¿ŸæŒ‡æ ‡æåº¦å¼‚å¸¸

#### 5. Packet Corruption (æ•°æ®åŒ…æŸå)

```
ç‰¹å¾æ¨¡å¼ï¼šé‡ä¼ æ¬¡æ•°â†‘ + ä¸¢åŒ…ç‡â†‘ + ä¿¡å·ä¸­ç­‰
å…¸å‹å€¼ï¼š[62, 0.38, 45, 0.095, 28, 55]
```

- **æˆå› **ï¼šä¼ è¾“é”™è¯¯ã€æ•°æ®å®Œæ•´æ€§é—®é¢˜
- **å½±å“**ï¼šæ•°æ®ä¼ è¾“ä¸å¯é ã€é¢‘ç¹é‡ä¼ 
- **æ£€æµ‹ç‰¹å¾**ï¼šä¸¢åŒ…ç›¸å…³æŒ‡æ ‡ä¸¥é‡å¼‚å¸¸

#### 6. Mixed Anomaly (æ··åˆå¼‚å¸¸)

```
ç‰¹å¾æ¨¡å¼ï¼šå¤šä¸ªä¸ç›¸å…³æŒ‡æ ‡åŒæ—¶å¼‚å¸¸
å…¸å‹å€¼ï¼š[45, 0.22, 95, 0.08, 75, 85]
```

- **æˆå› **ï¼šå¤æ‚æ•…éšœã€å¤šç§é—®é¢˜å¹¶å‘
- **å½±å“**ï¼šç³»ç»Ÿæ•´ä½“æ€§èƒ½ä¸¥é‡ä¸‹é™
- **æ£€æµ‹ç‰¹å¾**ï¼š6ä¸ªæŒ‡æ ‡éƒ½å‡ºç°ä¸åŒç¨‹åº¦å¼‚å¸¸

## ğŸ“Š è®­ç»ƒæ•°æ®ç”Ÿæˆç³»ç»Ÿ

### æ•°æ®ç”Ÿæˆç­–ç•¥

#### æ­£å¸¸æ•°æ®ç”Ÿæˆ (15000æ¡)

```python
# æ ¸å¿ƒå‚æ•°èŒƒå›´
signal_strength: 70-90 (ä¼˜ç§€ä¿¡å·è´¨é‡)
data_rate: 0.45-0.75 (æ­£å¸¸ä¼ è¾“é€Ÿç‡)
latency: 10-30ms (ä½å»¶è¿Ÿ)
packet_loss: 0.001-0.05 (æä½ä¸¢åŒ…)
cpu_usage: 5-30% (æ­£å¸¸è´Ÿè½½)
memory_usage: 30-70% (å¥åº·å†…å­˜ä½¿ç”¨)
```

#### å¼‚å¸¸æ•°æ®ç”Ÿæˆ (1800æ¡)

- **æ¯ç§å¼‚å¸¸ç±»å‹**ï¼š300æ¡æ ·æœ¬
- **æ•°æ®åˆ†å¸ƒ**ï¼šæ­£æ€åˆ†å¸ƒ + å™ªå£°æ³¨å…¥
- **ç‰¹å¾å…³è”**ï¼šæ¨¡æ‹ŸçœŸå®å¼‚å¸¸çš„å¤šæŒ‡æ ‡ç›¸å…³æ€§
- **æ ‡ç­¾å®Œæ•´**ï¼š6ç§å¼‚å¸¸ç±»å‹å®Œæ•´æ ‡æ³¨

### ç‰¹å¾æ˜ å°„ç®—æ³•

#### 11ç»´â†’6ç»´æ˜ å°„é€»è¾‘

```python
def convert_raw_to_6d_features(raw_data):
    # 1. ä¿¡å·å¼ºåº¦ï¼šç›´æ¥æ˜ å°„
    features[0] = raw_data['wlan0_wireless_quality']
  
    # 2. ä¼ è¾“é€Ÿç‡ï¼šå½’ä¸€åŒ–å¤„ç†
    avg_rate = (send_rate + recv_rate) / 2
    features[1] = min(avg_rate / 2000000.0, 1.0)
  
    # 3. ç½‘ç»œå»¶è¿Ÿï¼šå¤šæŒ‡æ ‡èåˆ
    features[2] = (ping_time + dns_time) / 2
  
    # 4. ä¸¢åŒ…ç‡ï¼šé‡ä¼ è¡¥å¿ç®—æ³•
    retrans_loss = min(retrans / 1000.0, 0.05)
    features[3] = packet_loss + retrans_loss
  
    # 5-6. ç³»ç»Ÿèµ„æºï¼šç›´æ¥æ˜ å°„
    features[4] = cpu_percent
    features[5] = memory_percent
```

## ğŸ”§ ä½¿ç”¨æŒ‡å—

### ç¯å¢ƒå‡†å¤‡

#### 1. ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**ï¼šLinux/Windows/macOS
- **Pythonç‰ˆæœ¬**ï¼š3.8+ (æ¨è3.9)
- **å†…å­˜è¦æ±‚**ï¼šæœ€å°‘4GBï¼Œæ¨è8GB
- **ç£ç›˜ç©ºé—´**ï¼šè‡³å°‘2GBå¯ç”¨ç©ºé—´

#### 2. ä¾èµ–å®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv ai_anomaly_detection
source ai_anomaly_detection/bin/activate  # Linux/macOS
# ai_anomaly_detection\Scripts\activate    # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### å®Œæ•´å·¥ä½œæµç¨‹

#### æ­¥éª¤1ï¼šç”Ÿæˆè®­ç»ƒæ•°æ®

```bash
# ç”Ÿæˆ6ç»´ç‰¹å¾è®­ç»ƒæ•°æ®
python3 scripts/generate_simple_6d_data.py

# éªŒè¯æ•°æ®ç”Ÿæˆ
ls -la data/6d_*.csv
```

**è¾“å‡ºæ–‡ä»¶**ï¼š

- `data/6d_normal_traffic.csv`ï¼š15000æ¡æ­£å¸¸æ•°æ®
- `data/6d_labeled_anomalies.csv`ï¼š1800æ¡å¼‚å¸¸æ•°æ®

#### æ­¥éª¤2ï¼šè®­ç»ƒAIæ¨¡å‹

```bash
# ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒè‡ªç¼–ç å™¨ (å¼‚å¸¸æ£€æµ‹)
python3 scripts/train_model.py autoencoder --data_path data/6d_normal_traffic.csv

# ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒåˆ†ç±»å™¨ (å¼‚å¸¸åˆ†ç±»)
python3 scripts/train_model.py classifier --data_path data/6d_labeled_anomalies.csv
```

#### æ­¥éª¤3ï¼šè¿è¡Œå¼‚å¸¸æ£€æµ‹

```bash
# äº¤äº’å¼æµ‹è¯• (æ‰‹åŠ¨è¾“å…¥æ•°æ®)
python3 scripts/interactive_tester.py

# è‡ªåŠ¨åŒ–æµ‹è¯• (ä½¿ç”¨é»˜è®¤æ•°æ®)
python3 scripts/interactive_tester.py --auto

# é¢„è®¾åœºæ™¯æµ‹è¯•
python3 scripts/test_scenarios.py
```

### é«˜çº§åŠŸèƒ½

#### ç‰¹å¾åˆ†æå·¥å…·

```bash
# åˆ†æå¼‚å¸¸ç‰¹å¾ç»„åˆ
python3 scripts/analyze_anomaly_features.py

# è°ƒè¯•ç‰¹å¾æå–è¿‡ç¨‹
python3 scripts/debug_feature_extraction.py

# å¿«é€ŸéªŒè¯ä¿®å¤
python3 scripts/quick_test_fix.py
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ä¸æµ‹è¯•ç»“æœ

### è‡ªç¼–ç å™¨æ€§èƒ½


| æŒ‡æ ‡         | æ•°å€¼      | è¯´æ˜             |
| ------------ | --------- | ---------------- |
| **è®­ç»ƒå‘¨æœŸ** | 53 epochs | æ—©åœæœºåˆ¶è‡ªåŠ¨ä¼˜åŒ– |
| **æœ€ç»ˆæŸå¤±** | 0.804     | æ”¶æ•›è‰¯å¥½         |
| **éªŒè¯æŸå¤±** | 0.733     | æ— è¿‡æ‹Ÿåˆç°è±¡     |
| **å¼‚å¸¸é˜ˆå€¼** | 1.8       | å®é™…åº”ç”¨ä¼˜åŒ–å   |
| **æ£€æµ‹æ—¶é—´** | <5ms      | å•æ¬¡æ¨ç†å»¶è¿Ÿ     |

### åˆ†ç±»å™¨æ€§èƒ½


| å¼‚å¸¸ç±»å‹               | ç²¾ç¡®ç‡    | å¬å›ç‡    | F1åˆ†æ•°    | æ”¯æŒæ•°  |
| ---------------------- | --------- | --------- | --------- | ------- |
| **connection_timeout** | 100%      | 100%      | 100%      | 60      |
| **mixed_anomaly**      | 100%      | 100%      | 100%      | 60      |
| **network_congestion** | 98%       | 98%       | 98%       | 60      |
| **packet_corruption**  | 98%       | 100%      | 99%       | 60      |
| **resource_overload**  | 100%      | 100%      | 100%      | 60      |
| **signal_degradation** | 98%       | 97%       | 97%       | 60      |
| **æ€»ä½“å‡†ç¡®ç‡**         | **99.2%** | **99.2%** | **99.2%** | **360** |

### å®é™…åº”ç”¨æ•ˆæœ

- âœ… **è¯¯æŠ¥ç‡**ï¼šä»100% â†’ 0%ï¼ˆé»˜è®¤æ•°æ®æµ‹è¯•ï¼‰
- âœ… **æ£€æµ‹ç²¾åº¦**ï¼š99.2%æµ‹è¯•å‡†ç¡®ç‡
- âœ… **å“åº”æ—¶é—´**ï¼š<15msç«¯åˆ°ç«¯æ£€æµ‹
- âœ… **èµ„æºæ¶ˆè€—**ï¼šå†…å­˜å ç”¨<100MB

## ğŸ—ï¸ é¡¹ç›®æ¶æ„å˜æ›´å†ç¨‹

### Version 1.0 â†’ 2.0 é‡å¤§æ¶æ„å‡çº§

#### é—®é¢˜è¯Šæ–­

**åŸå§‹é—®é¢˜**ï¼šè®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®æ ¼å¼å®Œå…¨ä¸åŒ¹é…

- âŒ è®­ç»ƒæ•°æ®ï¼š28ç»´äººå·¥ç‰¹å¾
- âŒ æµ‹è¯•æ•°æ®ï¼š11ç»´çœŸå®ç½‘ç»œæŒ‡æ ‡
- âŒ ç»“æœï¼š100%è¯¯æŠ¥ç‡

#### è§£å†³æ–¹æ¡ˆ

**æ¶æ„é‡æ„**ï¼šé™ç»´ä¼˜åŒ– + æ•°æ®æ ¼å¼ç»Ÿä¸€

- âœ… **ç‰¹å¾ç»´åº¦**ï¼š28ç»´ â†’ 6ç»´æ ¸å¿ƒç‰¹å¾
- âœ… **å¼‚å¸¸ç±»å‹**ï¼š8ç§ â†’ 6ç§ä¸»è¦ç±»å‹
- âœ… **æ•°æ®ç»Ÿä¸€**ï¼šè®­ç»ƒå’Œæµ‹è¯•å®Œå…¨åŒ¹é…
- âœ… **æ¨¡å‹ä¼˜åŒ–**ï¼š6â†’3â†’6è‡ªç¼–ç å™¨æ¶æ„

#### æŠ€æœ¯äº®ç‚¹

1. **æ™ºèƒ½ç‰¹å¾æ˜ å°„**ï¼š11ç»´åŸå§‹æŒ‡æ ‡â†’6ç»´æ ¸å¿ƒç‰¹å¾
2. **æ•°æ®ç”Ÿæˆä¼˜åŒ–**ï¼šåŸºäºçœŸå®æ¨¡å¼çš„ä»¿çœŸæ•°æ®
3. **é˜ˆå€¼åŠ¨æ€è°ƒæ•´**ï¼šä»1.743â†’1.8å®ç”¨ä¼˜åŒ–
4. **å¤šæŒ‡æ ‡èåˆ**ï¼šæ¯ç§å¼‚å¸¸çš„ç‰¹å¾"æŒ‡çº¹"è¯†åˆ«

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
ai-anomaly-detection/
â”œâ”€â”€ config/                          # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ system_config.json           # ç³»ç»Ÿæ ¸å¿ƒé…ç½®
â”œâ”€â”€ data/                            # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ 6d_normal_traffic.csv        # 6ç»´æ­£å¸¸æµé‡æ•°æ® (15000æ¡)
â”‚   â”œâ”€â”€ 6d_labeled_anomalies.csv     # 6ç»´å¼‚å¸¸æ•°æ® (1800æ¡)
â”‚   â””â”€â”€ simulation_inputs.json       # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
â”œâ”€â”€ models/                          # è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ autoencoder_model/           # è‡ªç¼–ç å™¨æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ saved_model.pb           # TensorFlowæ¨¡å‹æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ autoencoder_config.json  # æ¨¡å‹é…ç½®å’Œé˜ˆå€¼
â”‚   â”‚   â”œâ”€â”€ autoencoder_scaler.pkl   # ç‰¹å¾æ ‡å‡†åŒ–å™¨
â”‚   â”‚   â””â”€â”€ variables/               # æ¨¡å‹æƒé‡
â”‚   â””â”€â”€ error_classifier.pkl         # éšæœºæ£®æ—åˆ†ç±»å™¨
â”œâ”€â”€ scripts/                         # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ generate_simple_6d_data.py   # 6ç»´æ•°æ®ç”Ÿæˆå™¨ â­
â”‚   â”œâ”€â”€ train_model.py               # æ¨¡å‹è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ interactive_tester.py        # äº¤äº’å¼æµ‹è¯•å·¥å…· â­
â”‚   â”œâ”€â”€ test_scenarios.py            # é¢„è®¾åœºæ™¯æµ‹è¯•
â”‚   â”œâ”€â”€ analyze_anomaly_features.py  # ç‰¹å¾åˆ†æå·¥å…·
â”‚   â”œâ”€â”€ quick_test_fix.py            # å¿«é€ŸéªŒè¯å·¥å…·
â”‚   â””â”€â”€ debug_feature_extraction.py  # è°ƒè¯•å·¥å…·
â”œâ”€â”€ src/                             # æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ ai_models/                   # AIæ¨¡å‹æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ autoencoder_model.py     # æ·±åº¦è‡ªç¼–ç å™¨
â”‚   â”‚   â””â”€â”€ error_classifier.py     # éšæœºæ£®æ—åˆ†ç±»å™¨
â”‚   â”œâ”€â”€ anomaly_detector/            # å¼‚å¸¸æ£€æµ‹å¼•æ“
â”‚   â”‚   â””â”€â”€ anomaly_engine.py        # æ ¸å¿ƒæ£€æµ‹å¼•æ“
â”‚   â”œâ”€â”€ feature_processor/           # ç‰¹å¾å¤„ç†
â”‚   â”‚   â””â”€â”€ feature_extractor.py     # ç‰¹å¾æå–å™¨
â”‚   â”œâ”€â”€ logger/                      # æ—¥å¿—ç³»ç»Ÿ
â”‚   â””â”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”œâ”€â”€ guide/                           # ä½¿ç”¨æŒ‡å—
â”‚   â””â”€â”€ æ‰‹åŠ¨æµ‹è¯•æŒ‡å—.md              # è¯¦ç»†æµ‹è¯•è¯´æ˜
â”œâ”€â”€ requirements.txt                 # Pythonä¾èµ–
â””â”€â”€ README.md                        # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

### 5åˆ†é’Ÿå¿«é€Ÿä½“éªŒ

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository_url>
cd ai-anomaly-detection

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. ç”Ÿæˆæ•°æ®å¹¶è®­ç»ƒ (å¦‚æœæ¨¡å‹ä¸å­˜åœ¨)
python3 scripts/generate_simple_6d_data.py
python3 scripts/train_model.py autoencoder --data_path data/6d_normal_traffic.csv
python3 scripts/train_model.py classifier --data_path data/6d_labeled_anomalies.csv

# 4. è¿è¡Œæ£€æµ‹
python3 scripts/interactive_tester.py --auto
```

### é¢„æœŸè¾“å‡º

```
============ æ£€æµ‹ç»“æœ ============
çŠ¶æ€: ä¸€åˆ‡æ­£å¸¸

--- è¯¦ç»†æŠ€æœ¯ä¿¡æ¯ ---
æ¨¡å‹é‡æ„è¯¯å·®: 1.771107
æ¨¡å‹å¼‚å¸¸é˜ˆå€¼: 1.800000
====================================
```

## â“ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜è§£å†³

#### 1. æ¨¡å‹æ–‡ä»¶ç¼ºå¤±

```bash
# é”™è¯¯ï¼šFileNotFoundError: models/autoencoder_model
# è§£å†³ï¼šé‡æ–°è®­ç»ƒæ¨¡å‹
python3 scripts/train_model.py autoencoder --data_path data/6d_normal_traffic.csv
```

#### 2. ä¾èµ–ç‰ˆæœ¬å†²çª

```bash
# é”™è¯¯ï¼šImportErroræˆ–ç‰ˆæœ¬ä¸å…¼å®¹
# è§£å†³ï¼šé‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
rm -rf venv
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

#### 3. æ•°æ®æ ¼å¼é”™è¯¯

```bash
# é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘'label'åˆ—
# è§£å†³ï¼šé‡æ–°ç”Ÿæˆæ•°æ®
python3 scripts/generate_simple_6d_data.py
```

#### 4. é‡æ„è¯¯å·®å¼‚å¸¸

```bash
# é—®é¢˜ï¼šreconstruction_error > 1000000
# åŸå› ï¼šç‰¹å¾æ˜ å°„æˆ–æ¨¡å‹åŠ è½½é—®é¢˜
# è§£å†³ï¼šä½¿ç”¨å¿«é€ŸéªŒè¯å·¥å…·
python3 scripts/quick_test_fix.py
```


---

## ğŸ“Š é¡¹ç›®ç»Ÿè®¡

- **ä»£ç è¡Œæ•°**ï¼šçº¦3000è¡ŒPythonä»£ç 
- **æ¨¡å‹æ–‡ä»¶**ï¼š2ä¸ªAIæ¨¡å‹ï¼ˆè‡ªç¼–ç å™¨+åˆ†ç±»å™¨ï¼‰
- **è®­ç»ƒæ•°æ®**ï¼š16800æ¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®
- **æµ‹è¯•è¦†ç›–**ï¼š6ç§å¼‚å¸¸ç±»å‹å®Œæ•´è¦†ç›–
- **å“åº”æ—¶é—´**ï¼š<15msç«¯åˆ°ç«¯æ£€æµ‹å»¶è¿Ÿ
- **å‡†ç¡®ç‡**ï¼š99.2%æµ‹è¯•å‡†ç¡®ç‡
