import torch
import torch.nn as nn
import numpy as np
import joblib
import os
import subprocess

# ç‰¹å¾å·¥ç¨‹å±‚ï¼š11ç»´åŸå§‹è¾“å…¥ â†’ 6ç»´ç‰¹å¾
class FeatureEngineeringLayer(nn.Module):
    def __init__(self):
        super(FeatureEngineeringLayer, self).__init__()
        
    def forward(self, x):
        # è¾“å…¥x: [batch_size, 11]
        # æŒ‰ç…§READMEä¸­çš„è½¬æ¢å…¬å¼è¿›è¡Œç‰¹å¾å·¥ç¨‹
        
        # è¾“å…¥ç‰¹å¾ç´¢å¼• (åŸºäºREADMEä¸­çš„11ç»´åŸå§‹æ•°æ®)
        # 0: wlan0_wireless_quality, 1: wlan0_signal_level, 2: wlan0_noise_level
        # 3: wlan0_rx_packets, 4: wlan0_tx_packets, 5: wlan0_rx_bytes, 6: wlan0_tx_bytes  
        # 7: gateway_ping_time, 8: dns_resolution_time
        # 9: memory_usage_percent, 10: cpu_usage_percent
        
        # 1. å¹³å‡ä¿¡å·å¼ºåº¦ = (ä¿¡å·è´¨é‡ + |ä¿¡å·å¼ºåº¦|) / 20
        avg_signal_strength = (x[:, 0] + torch.abs(x[:, 1])) / 20.0
        
        # 2. å¹³å‡æ•°æ®ä¼ è¾“ç‡ = min((æ¥æ”¶å­—èŠ‚æ•° + å‘é€å­—èŠ‚æ•°) / 5000000, 1.0)
        avg_data_rate = torch.clamp((x[:, 5] + x[:, 6]) / 5000000.0, max=1.0)
        
        # 3. å¹³å‡ç½‘ç»œå»¶è¿Ÿ = (ç½‘å…³ping + DNSè§£ææ—¶é—´) / 2
        avg_latency = (x[:, 7] + x[:, 8]) / 2.0
        
        # 4. ä¸¢åŒ…ç‡ä¼°ç®— = max(0, (|å™ªå£°æ°´å¹³| - 70) / 200)
        packet_loss_rate = torch.clamp((torch.abs(x[:, 2]) - 70) / 200.0, min=0.0)
        
        # 5. ç³»ç»Ÿè´Ÿè½½ = (CPUä½¿ç”¨ç‡ + å†…å­˜ä½¿ç”¨ç‡) / 200
        system_load = (x[:, 10] + x[:, 9]) / 200.0
        
        # 6. ç½‘ç»œç¨³å®šæ€§ = min((æ¥æ”¶åŒ…æ•° + å‘é€åŒ…æ•°) / 50000, 1.0)
        network_stability = torch.clamp((x[:, 3] + x[:, 4]) / 50000.0, max=1.0)
        
        # å †å ä¸º6ç»´ç‰¹å¾å‘é‡
        features = torch.stack([
            avg_signal_strength,
            avg_data_rate, 
            avg_latency,
            packet_loss_rate,
            system_load,
            network_stability
        ], dim=1)
        
        return features

# ç«¯åˆ°ç«¯å¼‚å¸¸æ£€æµ‹ç½‘ç»œ (11ç»´è¾“å…¥)
class EndToEndAnomalyDetector(nn.Module):
    def __init__(self):
        super(EndToEndAnomalyDetector, self).__init__()
        
        # ç‰¹å¾å·¥ç¨‹å±‚: 11ç»´ â†’ 6ç»´
        self.feature_engineering = FeatureEngineeringLayer()
        
        # å¼‚å¸¸æ£€æµ‹ç½‘ç»œ: 6ç»´ â†’ 2åˆ†ç±»
        self.detector = nn.Sequential(
            nn.Linear(6, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(32, 16),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(16, 2)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # x: [batch_size, 11] åŸå§‹ç½‘ç»œç›‘æ§æ•°æ®
        features = self.feature_engineering(x)  # [batch_size, 6]
        detection_scores = self.detector(features)  # [batch_size, 2]
        return detection_scores

# ç«¯åˆ°ç«¯å¼‚å¸¸åˆ†ç±»ç½‘ç»œ (11ç»´è¾“å…¥)
class EndToEndAnomalyClassifier(nn.Module):
    def __init__(self, n_classes=6):
        super(EndToEndAnomalyClassifier, self).__init__()
        
        # ç‰¹å¾å·¥ç¨‹å±‚: 11ç»´ â†’ 6ç»´
        self.feature_engineering = FeatureEngineeringLayer()
        
        # å¼‚å¸¸åˆ†ç±»ç½‘ç»œ: 6ç»´ â†’ 6åˆ†ç±»
        self.classifier = nn.Sequential(
            nn.Linear(6, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(32, n_classes)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # x: [batch_size, 11] åŸå§‹ç½‘ç»œç›‘æ§æ•°æ®
        features = self.feature_engineering(x)  # [batch_size, 6]
        classification_scores = self.classifier(features)  # [batch_size, 6]
        return classification_scores

def convert_to_onnx(model, input_shape, output_path):
    """å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼"""
    print(f"è½¬æ¢æ¨¡å‹ä¸ºONNXæ ¼å¼: {output_path}")
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
    dummy_input = torch.randn(1, *input_shape)
    
    # è½¬æ¢ä¸ºONNX
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'},
                     'output': {0: 'batch_size'}}
    )
    
    print(f"âœ… ONNXæ¨¡å‹å·²ä¿å­˜: {output_path}")

def convert_to_dlc(onnx_path, dlc_path):
    """å°†ONNXæ¨¡å‹è½¬æ¢ä¸ºDLCæ ¼å¼"""
    print(f"è½¬æ¢ONNXä¸ºDLCæ ¼å¼: {dlc_path}")
    
    # è®¾ç½®å‘½ä»¤å‚æ•°
    cmd = [
        './2.26.2.240911/bin/x86_64-linux-clang/snpe-onnx-to-dlc',
        '--input_network', onnx_path,
        '--output_path', dlc_path,
        '--input_dim', 'input', '11'
    ]
    
    try:
        # æ‰§è¡Œè½¬æ¢
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print(f"âœ… DLCæ¨¡å‹å·²ç”Ÿæˆ: {dlc_path}")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        if os.path.exists(dlc_path):
            file_size = os.path.getsize(dlc_path)
            print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB")
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ DLCè½¬æ¢å¤±è´¥: {e}")
        print(f"é”™è¯¯è¾“å‡º: {e.stderr}")
        return False

def main():
    print("=== ç«¯åˆ°ç«¯æ¨¡å‹è½¬æ¢ä¸ºDLCæ ¼å¼ ===")
    print()
    
    # 1. åŠ è½½è®­ç»ƒå¥½çš„ç«¯åˆ°ç«¯æ¨¡å‹
    print("ğŸ”„ åŠ è½½è®­ç»ƒå¥½çš„ç«¯åˆ°ç«¯æ¨¡å‹...")
    
    # å¼‚å¸¸æ£€æµ‹æ¨¡å‹
    detector_model = EndToEndAnomalyDetector()
    detector_model.load_state_dict(torch.load('end_to_end_anomaly_detector.pth', map_location='cpu'))
    detector_model.eval()
    
    # å¼‚å¸¸åˆ†ç±»æ¨¡å‹
    classifier_model = EndToEndAnomalyClassifier(n_classes=6)
    classifier_model.load_state_dict(torch.load('end_to_end_anomaly_classifier.pth', map_location='cpu'))
    classifier_model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 2. è½¬æ¢ä¸ºONNXæ ¼å¼
    print("\nğŸ”„ è½¬æ¢ä¸ºONNXæ ¼å¼...")
    
    # å¼‚å¸¸æ£€æµ‹æ¨¡å‹ -> ONNX
    convert_to_onnx(detector_model, (11,), 'end_to_end_anomaly_detector.onnx')
    
    # å¼‚å¸¸åˆ†ç±»æ¨¡å‹ -> ONNX
    convert_to_onnx(classifier_model, (11,), 'end_to_end_anomaly_classifier.onnx')
    
    # 3. è½¬æ¢ä¸ºDLCæ ¼å¼
    print("\nğŸ”„ è½¬æ¢ä¸ºDLCæ ¼å¼...")
    
    # å¼‚å¸¸æ£€æµ‹æ¨¡å‹ -> DLC
    detector_success = convert_to_dlc('end_to_end_anomaly_detector.onnx', 'end_to_end_anomaly_detector.dlc')
    
    # å¼‚å¸¸åˆ†ç±»æ¨¡å‹ -> DLC
    classifier_success = convert_to_dlc('end_to_end_anomaly_classifier.onnx', 'end_to_end_anomaly_classifier.dlc')
    
    # 4. ç»“æœæ±‡æ€»
    print("\n=== è½¬æ¢ç»“æœæ±‡æ€» ===")
    
    if detector_success and classifier_success:
        print("âœ… æ‰€æœ‰æ¨¡å‹è½¬æ¢æˆåŠŸ!")
        print()
        print("ğŸ“ ç”Ÿæˆçš„DLCæ–‡ä»¶:")
        print("   - end_to_end_anomaly_detector.dlc    (å¼‚å¸¸æ£€æµ‹)")
        print("   - end_to_end_anomaly_classifier.dlc  (å¼‚å¸¸åˆ†ç±»)")
        print()
        print("ğŸ¯ **ç«¯åˆ°ç«¯æ–¹æ¡ˆä¼˜åŠ¿**:")
        print("   âœ… ç›´æ¥å¤„ç†11ç»´åŸå§‹ç½‘ç»œç›‘æ§æ•°æ®")
        print("   âœ… å†…ç½®ç‰¹å¾å·¥ç¨‹è½¬æ¢ï¼ˆ11ç»´ â†’ 6ç»´ï¼‰")
        print("   âœ… ç§»åŠ¨è®¾å¤‡æ— éœ€é¢å¤–é¢„å¤„ç†ä»£ç ")
        print("   âœ… å®Œæ•´çš„ä¸¤é˜¶æ®µå¼‚å¸¸æ£€æµ‹æµç¨‹")
        print("   âœ… çœŸæ­£çš„ç«¯åˆ°ç«¯éƒ¨ç½²è§£å†³æ–¹æ¡ˆ")
        
        # æ˜¾ç¤ºæ€»æ–‡ä»¶å¤§å°
        total_size = 0
        for filename in ['end_to_end_anomaly_detector.dlc', 'end_to_end_anomaly_classifier.dlc']:
            if os.path.exists(filename):
                size = os.path.getsize(filename)
                total_size += size
        
        print(f"   ğŸ“¦ æ€»DLCæ–‡ä»¶å¤§å°: {total_size / 1024:.1f} KB")
        
    else:
        print("âŒ éƒ¨åˆ†æ¨¡å‹è½¬æ¢å¤±è´¥")
        if not detector_success:
            print("   - å¼‚å¸¸æ£€æµ‹æ¨¡å‹è½¬æ¢å¤±è´¥")
        if not classifier_success:
            print("   - å¼‚å¸¸åˆ†ç±»æ¨¡å‹è½¬æ¢å¤±è´¥")

if __name__ == "__main__":
    main() 