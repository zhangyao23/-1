#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆ†ç±»å™¨åå·®è¯Šæ–­è„šæœ¬
åˆ†æä¸ºä»€ä¹ˆåˆ†ç±»å™¨åªèƒ½è¯†åˆ«ä¸€ç§å¼‚å¸¸ç±»å‹
"""

import numpy as np
import pandas as pd
import sys
import os
import joblib
from collections import Counter

# æ·»åŠ æºä»£ç è·¯å¾„
sys.path.append('src')
sys.path.append('.')

print('ğŸ” åˆ†ç±»å™¨åå·®é—®é¢˜è¯Šæ–­')
print('=' * 60)

# ========================================
# ç¬¬1æ­¥ï¼šåˆ†æè®­ç»ƒæ•°æ®åˆ†å¸ƒ
# ========================================
print('\nğŸ“Š ç¬¬1æ­¥ï¼šåˆ†æè®­ç»ƒæ•°æ®åˆ†å¸ƒ')
print('-' * 40)

try:
    # åŠ è½½è®­ç»ƒæ•°æ®
    df = pd.read_csv('data/improved_training_data_6d.csv')
    print(f'âœ… åŠ è½½è®­ç»ƒæ•°æ®: {df.shape}')
    
    # åˆ†æå¼‚å¸¸ç±»å‹åˆ†å¸ƒ
    anomaly_data = df[df['label'] == 1]
    print(f'ğŸ“ˆ å¼‚å¸¸æ ·æœ¬æ€»æ•°: {len(anomaly_data)}')
    
    if 'anomaly_type' in anomaly_data.columns:
        type_distribution = anomaly_data['anomaly_type'].value_counts()
        print(f'ğŸ·ï¸ å¼‚å¸¸ç±»å‹åˆ†å¸ƒ:')
        for anomaly_type, count in type_distribution.items():
            percentage = count / len(anomaly_data) * 100
            print(f'  {anomaly_type}: {count} æ ·æœ¬ ({percentage:.1f}%)')
            
        # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
        max_count = type_distribution.max()
        min_count = type_distribution.min()
        imbalance_ratio = max_count / min_count
        print(f'ğŸ“Š æ•°æ®ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1 (æœ€å¤š:æœ€å°‘)')
        
        if imbalance_ratio > 3:
            print('âš ï¸ æ•°æ®ä¸¥é‡ä¸å¹³è¡¡ï¼')
        elif imbalance_ratio > 1.5:
            print('âš ï¸ æ•°æ®è½»åº¦ä¸å¹³è¡¡')
        else:
            print('âœ… æ•°æ®åˆ†å¸ƒç›¸å¯¹å¹³è¡¡')
    else:
        print('âŒ è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰anomaly_typeåˆ—')
        
except Exception as e:
    print(f'âŒ åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥: {e}')

# ========================================
# ç¬¬2æ­¥ï¼šåˆ†ææ¨¡å‹æ–‡ä»¶å†…å®¹
# ========================================
print('\nğŸ”§ ç¬¬2æ­¥ï¼šåˆ†ææ¨¡å‹æ–‡ä»¶å†…å®¹')
print('-' * 40)

try:
    model_data = joblib.load('models/rf_classifier_improved.pkl')
    print(f'ğŸ“¦ æ¨¡å‹æ–‡ä»¶ç±»å‹: {type(model_data)}')
    
    if isinstance(model_data, dict):
        print('ğŸ“‹ æ¨¡å‹æ–‡ä»¶å†…å®¹:')
        for key, value in model_data.items():
            print(f'  {key}: {type(value)}')
            
            if key == 'classes':
                print(f'    æ”¯æŒçš„ç±»åˆ«: {value}')
            elif key == 'training_info':
                print(f'    è®­ç»ƒä¿¡æ¯: {value}')
            elif key == 'model' and hasattr(value, 'classes_'):
                print(f'    sklearnæ¨¡å‹ç±»åˆ«: {value.classes_}')
                print(f'    sklearnç‰¹å¾æ•°: {getattr(value, "n_features_in_", "æœªçŸ¥")}')
                
                # åˆ†æç‰¹å¾é‡è¦æ€§
                if hasattr(value, 'feature_importances_'):
                    feature_importance = value.feature_importances_
                    feature_names = ['avg_signal_strength', 'avg_data_rate', 'avg_latency', 
                                   'packet_loss_rate', 'system_load', 'network_stability']
                    print(f'    ç‰¹å¾é‡è¦æ€§:')
                    for i, (name, importance) in enumerate(zip(feature_names, feature_importance)):
                        print(f'      {name}: {importance:.4f}')
                        
except Exception as e:
    print(f'âŒ åˆ†ææ¨¡å‹æ–‡ä»¶å¤±è´¥: {e}')

# ========================================
# ç¬¬3æ­¥ï¼šåˆ†ææµ‹è¯•æ•°æ®åˆ†å¸ƒ
# ========================================
print('\nğŸ“ˆ ç¬¬3æ­¥ï¼šåˆ†ææµ‹è¯•æ•°æ®åˆ†å¸ƒ')
print('-' * 40)

try:
    # åŠ è½½æµ‹è¯•æ•°æ®
    df_test = pd.read_csv('data/improved_training_data_6d.csv')
    anomaly_test = df_test[df_test['label'] == 1]
    
    # éšæœºé€‰æ‹©ä¸åŒç±»å‹çš„å¼‚å¸¸æ ·æœ¬è¿›è¡Œæµ‹è¯•
    if 'anomaly_type' in anomaly_test.columns:
        unique_types = anomaly_test['anomaly_type'].unique()
        print(f'ğŸ¯ å¯ç”¨çš„å¼‚å¸¸ç±»å‹: {unique_types}')
        
        # ä¸ºæ¯ç§ç±»å‹é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬
        test_samples = {}
        for anomaly_type in unique_types:
            type_samples = anomaly_test[anomaly_test['anomaly_type'] == anomaly_type]
            if len(type_samples) > 0:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬ä½œä¸ºä»£è¡¨
                sample = type_samples.iloc[0]
                features = sample.drop(['label', 'anomaly_type']).values
                test_samples[anomaly_type] = features
                print(f'  {anomaly_type}: æ ·æœ¬èŒƒå›´ {features.min():.3f} - {features.max():.3f}')
    else:
        print('âŒ æµ‹è¯•æ•°æ®ä¸­æ²¡æœ‰anomaly_typeåˆ—')
        test_samples = {}
        
except Exception as e:
    print(f'âŒ åˆ†ææµ‹è¯•æ•°æ®å¤±è´¥: {e}')
    test_samples = {}

# ========================================
# ç¬¬4æ­¥ï¼šæµ‹è¯•åˆ†ç±»å™¨å¯¹ä¸åŒç±»å‹çš„é¢„æµ‹
# ========================================
print('\nğŸ§ª ç¬¬4æ­¥ï¼šæµ‹è¯•åˆ†ç±»å™¨å¯¹ä¸åŒç±»å‹çš„é¢„æµ‹')
print('-' * 40)

try:
    # ç®€å•æ—¥å¿—ç±»
    class SimpleLogger:
        def info(self, msg): print(f"[INFO] {msg}")
        def warning(self, msg): print(f"[WARNING] {msg}")
        def error(self, msg): print(f"[ERROR] {msg}")
        def debug(self, msg): print(f"[DEBUG] {msg}")

    from ai_models.error_classifier import ErrorClassifier
    
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier_config = {
        'model_path': 'models/rf_classifier_improved.pkl',
        'classes': ['connection_timeout', 'mixed_anomaly', 'network_congestion', 
                   'packet_corruption', 'resource_overload', 'signal_degradation'],
        'confidence_threshold': 0.7
    }
    
    classifier = ErrorClassifier(classifier_config, SimpleLogger())
    
    if test_samples:
        print('ğŸ” æµ‹è¯•ä¸åŒç±»å‹å¼‚å¸¸çš„åˆ†ç±»ç»“æœ:')
        prediction_counts = Counter()
        
        for true_type, features in test_samples.items():
            result = classifier.classify_error(features)
            predicted_type = result['predicted_class']
            confidence = result['confidence']
            
            prediction_counts[predicted_type] += 1
            
            print(f'  çœŸå®ç±»å‹: {true_type:20} â†’ é¢„æµ‹ç±»å‹: {predicted_type:20} (ç½®ä¿¡åº¦: {confidence:.3f})')
            
        print(f'\nğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:')
        for pred_type, count in prediction_counts.items():
            percentage = count / len(test_samples) * 100
            print(f'  {pred_type}: {count}/{len(test_samples)} ({percentage:.1f}%)')
            
        # åˆ†æé¢„æµ‹åå·®
        if len(prediction_counts) == 1:
            print(f'âŒ ä¸¥é‡é—®é¢˜ï¼šæ‰€æœ‰æ ·æœ¬éƒ½è¢«é¢„æµ‹ä¸º {list(prediction_counts.keys())[0]}')
        elif len(prediction_counts) < len(unique_types) / 2:
            print(f'âš ï¸ åå·®é—®é¢˜ï¼šåªé¢„æµ‹äº† {len(prediction_counts)} ç§ç±»å‹ï¼Œå®é™…æœ‰ {len(unique_types)} ç§')
        else:
            print(f'âœ… é¢„æµ‹å¤šæ ·æ€§æ­£å¸¸ï¼šé¢„æµ‹äº† {len(prediction_counts)} ç§ç±»å‹')
            
    else:
        print('âŒ æ— æ³•è·å–æµ‹è¯•æ ·æœ¬')
        
except Exception as e:
    print(f'âŒ åˆ†ç±»å™¨æµ‹è¯•å¤±è´¥: {e}')
    import traceback
    traceback.print_exc()

# ========================================
# ç¬¬5æ­¥ï¼šæ·±åº¦åˆ†æåˆ†ç±»å™¨å†³ç­–è¿‡ç¨‹
# ========================================
print('\nğŸ§  ç¬¬5æ­¥ï¼šæ·±åº¦åˆ†æåˆ†ç±»å™¨å†³ç­–è¿‡ç¨‹')
print('-' * 40)

try:
    if test_samples and len(test_samples) > 0:
        # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
        sample_type, sample_features = list(test_samples.items())[0]
        print(f'ğŸ” è¯¦ç»†åˆ†ææ ·æœ¬: {sample_type}')
        print(f'ğŸ“Š ç‰¹å¾å€¼: {sample_features}')
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        features_reshaped = sample_features.reshape(1, -1)
        probabilities = classifier.classifier.predict_proba(features_reshaped)[0]
        
        print(f'ğŸ¯ å„ç±»åˆ«é¢„æµ‹æ¦‚ç‡:')
        class_names = classifier.label_encoder.classes_
        prob_pairs = list(zip(class_names, probabilities))
        prob_pairs.sort(key=lambda x: x[1], reverse=True)
        
        for class_name, prob in prob_pairs:
            stars = 'â˜…' * int(prob * 10)
            print(f'  {class_name:20}: {prob:.4f} {stars}')
            
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„æ¦‚ç‡åå·®
        max_prob = max(probabilities)
        second_max_prob = sorted(probabilities, reverse=True)[1]
        prob_gap = max_prob - second_max_prob
        
        print(f'\nğŸ“ˆ æ¦‚ç‡åˆ†æ:')
        print(f'  æœ€é«˜æ¦‚ç‡: {max_prob:.4f}')
        print(f'  ç¬¬äºŒé«˜æ¦‚ç‡: {second_max_prob:.4f}')
        print(f'  æ¦‚ç‡å·®è·: {prob_gap:.4f}')
        
        if prob_gap > 0.8:
            print('âŒ æåº¦åå‘å•ä¸€ç±»åˆ«ï¼')
        elif prob_gap > 0.5:
            print('âš ï¸ åå‘å•ä¸€ç±»åˆ«')
        else:
            print('âœ… æ¦‚ç‡åˆ†å¸ƒç›¸å¯¹åˆç†')
            
except Exception as e:
    print(f'âŒ å†³ç­–åˆ†æå¤±è´¥: {e}')

# ========================================
# è¯Šæ–­æ€»ç»“
# ========================================
print('\n' + '=' * 60)
print('ğŸ è¯Šæ–­æ€»ç»“')
print('=' * 60)

print('''
å¯èƒ½çš„é—®é¢˜åŸå› ï¼š

1ï¸âƒ£ è®­ç»ƒæ•°æ®ä¸å¹³è¡¡
   - æŸç§å¼‚å¸¸ç±»å‹æ ·æœ¬è¿‡å¤š
   - æ¨¡å‹å­¦ä¹ åå‘å¤šæ•°ç±»

2ï¸âƒ£ ç‰¹å¾ç©ºé—´é‡å 
   - ä¸åŒå¼‚å¸¸ç±»å‹çš„ç‰¹å¾ç›¸ä¼¼
   - åˆ†ç±»å™¨éš¾ä»¥åŒºåˆ†

3ï¸âƒ£ æ¨¡å‹è¿‡æ‹Ÿåˆ
   - è®­ç»ƒæ—¶è¿‡åº¦æ‹ŸåˆæŸä¸ªç±»åˆ«
   - æ³›åŒ–èƒ½åŠ›ä¸è¶³

4ï¸âƒ£ ç‰¹å¾å·¥ç¨‹é—®é¢˜
   - ç‰¹å¾å¯¹æŸäº›å¼‚å¸¸ç±»å‹ä¸æ•æ„Ÿ
   - éœ€è¦æ›´å¤šåŒºåˆ†æ€§ç‰¹å¾

5ï¸âƒ£ é˜ˆå€¼è®¾ç½®é—®é¢˜
   - ç½®ä¿¡åº¦é˜ˆå€¼è¿‡é«˜/è¿‡ä½
   - å½±å“åˆ†ç±»å†³ç­–
''')

print('ğŸ”¬ è¯Šæ–­å®Œæˆï¼æ ¹æ®ä»¥ä¸Šç»“æœåˆ†æå…·ä½“é—®é¢˜...') 