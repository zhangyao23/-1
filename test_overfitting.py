#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜
"""
import torch
import numpy as np
import joblib
from train_separate_models import AnomalyDetector, AnomalyClassifier

def test_overfitting():
    print("ğŸ” æµ‹è¯•æ¨¡å‹è¿‡æ‹Ÿåˆæƒ…å†µ")
    print("=" * 50)
    
    # åŠ è½½æ¨¡å‹
    detector = AnomalyDetector()
    detector.load_state_dict(torch.load("anomaly_detector.pth", map_location='cpu'))
    detector.eval()
    
    classifier = AnomalyClassifier()
    classifier.load_state_dict(torch.load("anomaly_classifier.pth", map_location='cpu'))
    classifier.eval()
    
    # åŠ è½½æ ‡å‡†åŒ–å™¨
    scaler = joblib.load('separate_models_scaler.pkl')
    
    # æµ‹è¯•1: è¾¹ç•Œæ¨¡ç³Šæ•°æ®
    print("\nğŸ“Š æµ‹è¯•1: è¾¹ç•Œæ¨¡ç³Šæ•°æ®")
    boundary_tests = [
        # æ¥è¿‘æ­£å¸¸ä½†å¯èƒ½å¼‚å¸¸çš„æ•°æ®
        [65.0, -65.0, -85.0, 16000, 13000, 3500000, 2800000, 45.0, 60.0, 45.0, 30.0],
        [70.0, -60.0, -80.0, 17000, 14000, 3800000, 3200000, 50.0, 70.0, 50.0, 35.0],
        [75.0, -55.0, -75.0, 18000, 15000, 4200000, 3600000, 55.0, 80.0, 55.0, 40.0],
    ]
    
    for i, test_data in enumerate(boundary_tests):
        print(f"\nè¾¹ç•Œæµ‹è¯• {i+1}:")
        input_scaled = scaler.transform([test_data])
        input_tensor = torch.FloatTensor(input_scaled)
        
        with torch.no_grad():
            detection_output = detector(input_tensor)
            detection_probs = torch.softmax(detection_output, dim=1)
            is_anomaly = torch.argmax(detection_probs, dim=1).item()
            confidence = torch.max(detection_probs, dim=1)[0].item()
            
            print(f"  æ£€æµ‹ç»“æœ: {'å¼‚å¸¸' if is_anomaly == 1 else 'æ­£å¸¸'}")
            print(f"  ç½®ä¿¡åº¦: {confidence:.4f}")
            print(f"  æ¦‚ç‡åˆ†å¸ƒ: {detection_probs[0].detach().numpy()}")
    
    # æµ‹è¯•2: æ··åˆç‰¹å¾æ•°æ®
    print("\nğŸ“Š æµ‹è¯•2: æ··åˆç‰¹å¾æ•°æ®")
    mixed_tests = [
        # éƒ¨åˆ†ç‰¹å¾å¼‚å¸¸ï¼Œéƒ¨åˆ†æ­£å¸¸
        [85.0, -45.0, -90.0, 15000, 12000, 3000000, 2500000, 120.0, 250.0, 40.0, 25.0],  # å»¶è¿Ÿå¼‚å¸¸
        [60.0, -75.0, -70.0, 15000, 12000, 3000000, 2500000, 25.0, 30.0, 85.0, 75.0],   # ä¿¡å·+èµ„æºå¼‚å¸¸
        [75.0, -50.0, -90.0, 8000, 6000, 1500000, 1200000, 25.0, 30.0, 40.0, 25.0],     # æµé‡å¼‚å¸¸
    ]
    
    for i, test_data in enumerate(mixed_tests):
        print(f"\næ··åˆæµ‹è¯• {i+1}:")
        input_scaled = scaler.transform([test_data])
        input_tensor = torch.FloatTensor(input_scaled)
        
        with torch.no_grad():
            detection_output = detector(input_tensor)
            detection_probs = torch.softmax(detection_output, dim=1)
            is_anomaly = torch.argmax(detection_probs, dim=1).item()
            confidence = torch.max(detection_probs, dim=1)[0].item()
            
            print(f"  æ£€æµ‹ç»“æœ: {'å¼‚å¸¸' if is_anomaly == 1 else 'æ­£å¸¸'}")
            print(f"  ç½®ä¿¡åº¦: {confidence:.4f}")
            
            if is_anomaly == 1:
                classification_output = classifier(input_tensor)
                classification_probs = torch.softmax(classification_output, dim=1)
                predicted_class = torch.argmax(classification_probs, dim=1).item()
                class_confidence = torch.max(classification_probs, dim=1)[0].item()
                
                anomaly_types = ["wifi_degradation", "network_latency", "connection_instability", 
                               "bandwidth_congestion", "system_stress", "dns_issues"]
                predicted_type = anomaly_types[predicted_class] if predicted_class < len(anomaly_types) else f"æœªçŸ¥({predicted_class})"
                
                print(f"  åˆ†ç±»ç»“æœ: {predicted_type}")
                print(f"  åˆ†ç±»ç½®ä¿¡åº¦: {class_confidence:.4f}")
    
    # æµ‹è¯•3: å™ªå£°æ•°æ®
    print("\nğŸ“Š æµ‹è¯•3: å™ªå£°æ•°æ®")
    # åœ¨æ­£å¸¸æ•°æ®åŸºç¡€ä¸Šæ·»åŠ å™ªå£°
    base_normal = [85.0, -45.0, -90.0, 15000, 12000, 3000000, 2500000, 25.0, 30.0, 40.0, 25.0]
    
    for noise_level in [0.05, 0.1, 0.15, 0.2]:
        print(f"\nå™ªå£°æ°´å¹³ {noise_level*100}%:")
        noisy_data = []
        for val in base_normal:
            noise = np.random.normal(0, abs(val) * noise_level)
            noisy_data.append(val + noise)
        
        input_scaled = scaler.transform([noisy_data])
        input_tensor = torch.FloatTensor(input_scaled)
        
        with torch.no_grad():
            detection_output = detector(input_tensor)
            detection_probs = torch.softmax(detection_output, dim=1)
            is_anomaly = torch.argmax(detection_probs, dim=1).item()
            confidence = torch.max(detection_probs, dim=1)[0].item()
            
            print(f"  æ£€æµ‹ç»“æœ: {'å¼‚å¸¸' if is_anomaly == 1 else 'æ­£å¸¸'}")
            print(f"  ç½®ä¿¡åº¦: {confidence:.4f}")
    
    print("\nğŸ¯ è¿‡æ‹Ÿåˆåˆ†ææ€»ç»“:")
    print("å¦‚æœæ¨¡å‹åœ¨è¾¹ç•Œæ•°æ®å’Œæ··åˆç‰¹å¾ä¸Šè¡¨ç°è¿‡äºç¡®å®šï¼ˆç½®ä¿¡åº¦>0.9ï¼‰ï¼Œ")
    print("å¯èƒ½è¡¨æ˜å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ã€‚")
    print("å»ºè®®:")
    print("1. å¢åŠ æ•°æ®å¤æ‚åº¦")
    print("2. æ·»åŠ æ›´å¤šæ­£åˆ™åŒ–")
    print("3. ä½¿ç”¨äº¤å‰éªŒè¯")
    print("4. æµ‹è¯•çœŸå®ç½‘ç»œæ•°æ®")

if __name__ == "__main__":
    test_overfitting() 