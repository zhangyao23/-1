#!/usr/bin/env python3
"""
SNPEæ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•AIç½‘ç»œå¼‚å¸¸æ£€æµ‹ç¨‹åºåœ¨SNPEç¯å¢ƒä¸­çš„æ€§èƒ½è¡¨ç°
"""

import os
import sys
import subprocess
import json
import time
import statistics
from pathlib import Path

class SNPEPerformanceTester:
    def __init__(self):
        self.project_root = Path.cwd()
        self.executable = self.project_root / "dlc_mobile_inference"
        self.results = {"performance_tests": [], "summary": {}}
        
    def log_performance(self, test_name, avg_time_ms, min_time_ms, max_time_ms, 
                       std_dev_ms, throughput_fps, details=""):
        """è®°å½•æ€§èƒ½æµ‹è¯•ç»“æœ"""
        result = {
            "test_name": test_name,
            "avg_time_ms": avg_time_ms,
            "min_time_ms": min_time_ms,
            "max_time_ms": max_time_ms,
            "std_dev_ms": std_dev_ms,
            "throughput_fps": throughput_fps,
            "details": details
        }
        self.results["performance_tests"].append(result)
        
        print(f"ğŸ“Š {test_name}")
        print(f"   å¹³å‡å»¶è¿Ÿ: {avg_time_ms:.1f}ms")
        print(f"   å»¶è¿ŸèŒƒå›´: {min_time_ms:.1f}-{max_time_ms:.1f}ms")
        print(f"   æ ‡å‡†å·®: {std_dev_ms:.1f}ms")
        print(f"   ååé‡: {throughput_fps:.1f} FPS")
        if details:
            print(f"   è¯¦æƒ…: {details}")
        print()
        
    def run_inference_batch(self, test_name, input_file, num_runs=20):
        """è¿è¡Œæ‰¹é‡æ¨ç†æµ‹è¯•"""
        if not self.executable.exists():
            print(f"âŒ å¯æ‰§è¡Œæ–‡ä»¶ä¸å­˜åœ¨: {self.executable}")
            return None
            
        input_path = self.project_root / input_file
        if not input_path.exists():
            print(f"âŒ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {input_path}")
            return None
            
        print(f"ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•: {test_name}")
        print(f"   è¿è¡Œæ¬¡æ•°: {num_runs}")
        print(f"   æµ‹è¯•æ•°æ®: {input_file}")
        
        times = []
        successful_runs = 0
        
        for i in range(num_runs):
            start_time = time.time()
            try:
                result = subprocess.run([
                    str(self.executable),
                    "realistic_end_to_end_anomaly_detector.dlc",
                    "realistic_end_to_end_anomaly_classifier.dlc",
                    str(input_path)
                ], capture_output=True, text=True, timeout=10)
                
                end_time = time.time()
                inference_time_ms = (end_time - start_time) * 1000
                
                if result.returncode == 0:
                    times.append(inference_time_ms)
                    successful_runs += 1
                    if (i + 1) % 5 == 0:
                        print(f"   è¿›åº¦: {i+1}/{num_runs} ({inference_time_ms:.1f}ms)")
                else:
                    print(f"   è¿è¡Œ {i+1} å¤±è´¥")
                    
            except subprocess.TimeoutExpired:
                print(f"   è¿è¡Œ {i+1} è¶…æ—¶")
            except Exception as e:
                print(f"   è¿è¡Œ {i+1} å¼‚å¸¸: {str(e)[:30]}")
        
        if len(times) < num_runs * 0.8:  # æˆåŠŸç‡ä½äº80%
            print(f"âš ï¸  æˆåŠŸç‡è¿‡ä½: {len(times)}/{num_runs}")
            return None
            
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = statistics.mean(times)
        min_time = min(times)
        max_time = max(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0
        throughput_fps = 1000 / avg_time if avg_time > 0 else 0
        
        details = f"æˆåŠŸç‡: {successful_runs}/{num_runs}"
        
        self.log_performance(test_name, avg_time, min_time, max_time, 
                           std_dev, throughput_fps, details)
        
        return {
            "avg_time_ms": avg_time,
            "throughput_fps": throughput_fps,
            "success_rate": successful_runs / num_runs
        }
    
    def test_different_scenarios(self):
        """æµ‹è¯•ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½"""
        print("ğŸ¯ å¼€å§‹å¤šåœºæ™¯æ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        scenarios = [
            ("æ­£å¸¸ç½‘ç»œçŠ¶å†µ", "normal_input.bin"),
            ("WiFiä¿¡å·è¡°å‡", "wifi_degradation_input.bin"),
            ("ç½‘ç»œå»¶è¿Ÿå¼‚å¸¸", "network_latency_input.bin"),
            ("è¿æ¥ä¸­æ–­å¼‚å¸¸", "connection_interruption_input.bin"),
            ("DNSè§£æå¼‚å¸¸", "dns_resolution_input.bin"),
            ("å¸¦å®½é™åˆ¶å¼‚å¸¸", "bandwidth_limitation_input.bin")
        ]
        
        results = []
        
        for scenario_name, input_file in scenarios:
            result = self.run_inference_batch(scenario_name, input_file, num_runs=10)
            if result:
                results.append({
                    "scenario": scenario_name,
                    "result": result
                })
        
        return results
    
    def test_cold_vs_warm_start(self):
        """æµ‹è¯•å†·å¯åŠ¨ vs çƒ­å¯åŠ¨æ€§èƒ½"""
        print("ğŸ”¥ æµ‹è¯•å†·å¯åŠ¨ vs çƒ­å¯åŠ¨æ€§èƒ½")
        print("=" * 60)
        
        # å†·å¯åŠ¨æµ‹è¯• - æ€æ­»æ‰€æœ‰ç›¸å…³è¿›ç¨‹åé‡æ–°å¼€å§‹
        print("â„ï¸  å†·å¯åŠ¨æµ‹è¯•...")
        subprocess.run(["pkill", "-f", "dlc_mobile_inference"], capture_output=True)
        time.sleep(2)  # ç¡®ä¿è¿›ç¨‹å®Œå…¨åœæ­¢
        
        cold_start_result = self.run_inference_batch("å†·å¯åŠ¨", "normal_input.bin", num_runs=5)
        
        # çƒ­å¯åŠ¨æµ‹è¯• - è¿ç»­è¿è¡Œ
        print("ğŸ”¥ çƒ­å¯åŠ¨æµ‹è¯•...")
        warm_start_result = self.run_inference_batch("çƒ­å¯åŠ¨", "normal_input.bin", num_runs=10)
        
        if cold_start_result and warm_start_result:
            improvement = ((cold_start_result["avg_time_ms"] - warm_start_result["avg_time_ms"]) 
                         / cold_start_result["avg_time_ms"] * 100)
            print(f"ğŸ“ˆ çƒ­å¯åŠ¨æ€§èƒ½æå‡: {improvement:.1f}%")
            
        return cold_start_result, warm_start_result
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ§  æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("=" * 60)
        
        try:
            # å¯åŠ¨æ¨ç†è¿›ç¨‹å¹¶ç›‘æ§å†…å­˜
            process = subprocess.Popen([
                str(self.executable),
                "realistic_end_to_end_anomaly_detector.dlc",
                "realistic_end_to_end_anomaly_classifier.dlc",
                str(self.project_root / "normal_input.bin")
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # è·å–è¿›ç¨‹å†…å­˜ä¿¡æ¯
            pid = process.pid
            time.sleep(0.1)  # ç­‰å¾…è¿›ç¨‹å¯åŠ¨
            
            try:
                memory_info = subprocess.run(
                    ["ps", "-p", str(pid), "-o", "pid,vsz,rss,pmem,cmd"],
                    capture_output=True, text=True, timeout=5
                )
                
                if memory_info.returncode == 0:
                    lines = memory_info.stdout.strip().split('\n')
                    if len(lines) > 1:
                        fields = lines[1].split()
                        vsz_kb = int(fields[1])  # è™šæ‹Ÿå†…å­˜ KB
                        rss_kb = int(fields[2])  # ç‰©ç†å†…å­˜ KB
                        cpu_percent = fields[3]   # CPUç™¾åˆ†æ¯”
                        
                        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ:")
                        print(f"   è™šæ‹Ÿå†…å­˜: {vsz_kb/1024:.1f} MB")
                        print(f"   ç‰©ç†å†…å­˜: {rss_kb/1024:.1f} MB")
                        print(f"   CPUä½¿ç”¨ç‡: {cpu_percent}%")
                        
                        self.results["memory_usage"] = {
                            "virtual_memory_mb": vsz_kb / 1024,
                            "physical_memory_mb": rss_kb / 1024,
                            "cpu_percent": cpu_percent
                        }
                
            except Exception as e:
                print(f"âš ï¸  å†…å­˜ç›‘æ§å¤±è´¥: {str(e)}")
                
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            process.wait(timeout=10)
            
        except Exception as e:
            print(f"âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {str(e)}")
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š")
        print("=" * 60)
        
        if not self.results["performance_tests"]:
            print("âŒ æ— æ€§èƒ½æµ‹è¯•æ•°æ®")
            return
            
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        all_avg_times = [test["avg_time_ms"] for test in self.results["performance_tests"]]
        all_throughputs = [test["throughput_fps"] for test in self.results["performance_tests"]]
        
        self.results["summary"] = {
            "total_tests": len(self.results["performance_tests"]),
            "overall_avg_latency_ms": statistics.mean(all_avg_times),
            "best_latency_ms": min(all_avg_times),
            "worst_latency_ms": max(all_avg_times),
            "overall_avg_throughput_fps": statistics.mean(all_throughputs),
            "best_throughput_fps": max(all_throughputs)
        }
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.project_root / "snpe_performance_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºæ‘˜è¦
        summary = self.results["summary"]
        print(f"ğŸ“‹ æ€§èƒ½æ‘˜è¦:")
        print(f"   æµ‹è¯•åœºæ™¯æ•°: {summary['total_tests']}")
        print(f"   å¹³å‡å»¶è¿Ÿ: {summary['overall_avg_latency_ms']:.1f}ms")
        print(f"   æœ€ä½³å»¶è¿Ÿ: {summary['best_latency_ms']:.1f}ms")
        print(f"   æœ€å·®å»¶è¿Ÿ: {summary['worst_latency_ms']:.1f}ms")
        print(f"   å¹³å‡ååé‡: {summary['overall_avg_throughput_fps']:.1f} FPS")
        print(f"   æœ€é«˜ååé‡: {summary['best_throughput_fps']:.1f} FPS")
        
        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æ€§èƒ½è¯„çº§
        avg_latency = summary['overall_avg_latency_ms']
        if avg_latency < 20:
            grade = "ä¼˜ç§€ (< 20ms)"
        elif avg_latency < 50:
            grade = "è‰¯å¥½ (< 50ms)"
        elif avg_latency < 100:
            grade = "ä¸€èˆ¬ (< 100ms)"
        else:
            grade = "éœ€è¦ä¼˜åŒ– (>= 100ms)"
            
        print(f"ğŸ† æ€§èƒ½è¯„çº§: {grade}")
        
    def run_full_performance_test(self):
        """è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ SNPEæ€§èƒ½æµ‹è¯•å¼€å§‹")
        print("=" * 60)
        
        # 1. å¤šåœºæ™¯æ€§èƒ½æµ‹è¯•
        scenario_results = self.test_different_scenarios()
        
        # 2. å†·çƒ­å¯åŠ¨å¯¹æ¯”
        cold_result, warm_result = self.test_cold_vs_warm_start()
        
        # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
        self.test_memory_usage()
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        self.generate_performance_report()
        
        return len(scenario_results) > 0

def main():
    tester = SNPEPerformanceTester()
    success = tester.run_full_performance_test()
    
    if success:
        print("\nâœ… SNPEæ€§èƒ½æµ‹è¯•å®Œæˆ")
    else:
        print("\nâŒ SNPEæ€§èƒ½æµ‹è¯•å¤±è´¥")
        
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main() 